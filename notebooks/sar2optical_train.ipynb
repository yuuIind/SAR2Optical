{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2008381,"sourceType":"datasetVersion","datasetId":1201791}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Set Up Logging For Experimentation ","metadata":{}},{"cell_type":"markdown","source":"Comet ML provides a convenient API to log experiments in a easy to follow and structured way. It is also free for individuals. There are also other choices, you can replace it with your choice of platform.","metadata":{}},{"cell_type":"markdown","source":"If Comet api is not installed, run the following line to install it","metadata":{}},{"cell_type":"code","source":"%pip install comet_ml -q","metadata":{"execution":{"iopub.status.busy":"2024-11-05T18:56:04.176771Z","iopub.execute_input":"2024-11-05T18:56:04.177239Z","iopub.status.idle":"2024-11-05T18:56:39.345602Z","shell.execute_reply.started":"2024-11-05T18:56:04.177196Z","shell.execute_reply":"2024-11-05T18:56:39.344033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Login using your api key.\n\nDo not hardcode you api key, use an configuration file or use secrets if you are using colabs or kaggle","metadata":{}},{"cell_type":"code","source":"# Log in to comet ml for experiment logging\n# Uncomment depending on the platform you are running: Kaggle, Colab\nfrom kaggle_secrets import UserSecretsClient\n#from google.colab import userdata\n\nimport comet_ml\n\napi_key = UserSecretsClient().get_secret('COMET_API_KEY')\n#api_key = userdata.get('COMET_API_KEY')\n\n# If you are running on local, run `comet_ml.login()` line only, it will prompt \n# you for your API key which, once provided, is automatically accessed from a \n# configuration file\ncomet_ml.login(api_key=api_key)","metadata":{"execution":{"iopub.status.busy":"2024-11-05T18:56:48.376629Z","iopub.execute_input":"2024-11-05T18:56:48.377785Z","iopub.status.idle":"2024-11-05T18:56:49.768245Z","shell.execute_reply.started":"2024-11-05T18:56:48.377729Z","shell.execute_reply":"2024-11-05T18:56:49.766979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create experiment object\nexperiment = comet_ml.start(project_name=\"SAR-2-Optical\")","metadata":{"execution":{"iopub.status.busy":"2024-11-05T18:59:47.797764Z","iopub.execute_input":"2024-11-05T18:59:47.798296Z","iopub.status.idle":"2024-11-05T18:59:52.903875Z","shell.execute_reply.started":"2024-11-05T18:59:47.798233Z","shell.execute_reply":"2024-11-05T18:59:52.901870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import json\nimport random\nfrom pathlib import Path\nfrom enum import Enum\nfrom typing import Tuple, List, Optional, Callable, Union, Literal\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import v2","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:08:01.958219Z","iopub.execute_input":"2024-11-06T11:08:01.958681Z","iopub.status.idle":"2024-11-06T11:08:07.932580Z","shell.execute_reply.started":"2024-11-06T11:08:01.958620Z","shell.execute_reply":"2024-11-06T11:08:07.931134Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Prepare The Dataset","metadata":{}},{"cell_type":"code","source":"class SplitType(Enum):\n    \"\"\"Enumeration for dataset split types\"\"\"\n    TRAIN = 'train'\n    VAL = 'val'\n    TEST = 'test'\n\n\nclass Sentinel(Dataset):\n    \"\"\"\n    A PyTorch Dataset for handling Sentinel-1&2 Image Pairs.\n    \n    This dataset assumes a directory structure of:\n    root_dir/\n        category1/\n            s1/\n                image1.png\n                image2.png\n            s2/\n                image1.png\n                image2.png\n        category2/\n            ...\n    \n    This class has support for train/val/test splits. When `split_type` is `None`, \n    uses the complete dataset. When `split_type` is specified \n    (``'train'``, ``'val'``, ``'test'``), the dataset can be split using:\n\n    1. A split that defines which images belong to which split\n    2. Random splitting with a specified ratio\n\n    Args:\n        root_dir (str | Path): Root directory containing the dataset\n        split_type (str | None): Which split to use ('train', 'val', 'test') or None for full dataset\n        transform (callable, optional): Transform to apply to both SAR and optical images\n        split_mode (str, optional): How to split the dataset ('random', 'split')\n        split_ratio (Tuple[float, float, float], optional): Ratio for train/val/test splits\n        split_file (str | Path, optional): predefined the splits\n        seed (int, optional): Random seed for reproducible splitting\n        \n    Attributes:\n        root_dir (Path): Path to the dataset root directory\n        transform (callable): Transform pipeline for the images\n        image_pairs (List[Tuple[Path, Path]]): List of paired image paths (SAR, optical)\n    \"\"\"\n    def __init__(self,\n                 root_dir: Union[str, Path],\n                 split_type: Optional[str] = None,\n                 transform: Optional[Callable] = None,\n                 split_mode: Literal['random', 'split'] = 'random',\n                 split_ratio: Tuple[float, float, float] = (0.7, 0.15, 0.15),\n                 split_file: Optional[Union[str, Path]] = None,\n                 seed: int = 42):\n        self.root_dir = Path(root_dir)\n        if not self.root_dir.exists():\n            raise FileNotFoundError(f\"Dataset root directory not found: {self.root_dir}\")\n        \n        # Convert string split_type to enum if provided\n        self.split_type = SplitType(split_type) if split_type else None\n\n        # Default transform pipeline\n        self.transform = transform if transform else v2.Compose([\n            v2.ToImage(),\n            v2.ToDtype(torch.float32, scale=True)\n        ])\n\n        # Collect image pairs\n        self.all_image_pairs = self._collect_images()\n\n        # Apply split if specified\n        if split_type:\n            if split_mode == 'split' and split_file:\n                self.image_pairs = self._apply_predefined_split(split_file)\n            elif split_mode == 'random':\n                self.image_pairs = self._apply_random_split(split_ratio, seed)\n            else:\n                raise ValueError(\"Invalid split configuration. Use either 'split' with a split_file or 'random' with split_ratio\")\n        else:\n            # If no split type specified, use all images\n            self.image_pairs = self.all_image_pairs\n\n        print(f'Total image pairs found: {len(self)}')\n\n    def _collect_images(self) -> List[Tuple[Path, Path]]:\n        \"\"\"\n        Collects paired SAR (s1) and optical (s2) image paths from the dataset directory.\n            \n        Returns:\n            List[Tuple[Path, Path]]: List of (SAR image path, optical image path) pairs\n        \"\"\"\n        image_pairs = []\n        \n        # Iterate through category subdirectories\n        for category in self.root_dir.iterdir():\n            # Check if it's a directory\n            if not category.is_dir():\n                continue\n\n            s1_path = category / 's1'\n            s2_path = category / 's2'\n            \n            if not (s1_path.is_dir() and s2_path.is_dir()):\n                # print(f\"Missing s1 or s2 subdirectory in category: {category.name}\")\n                continue\n\n            # Collect pairs\n            for s1_file in s1_path.glob('*.png'):\n                # Convert SAR filename to optical filename\n                # e.g. 'ROIs1970_fall_s1_13_p265.png' -> 'ROIs1970_fall_s2_13_p265.png'\n                s2_filename = list(s1_file.name.split('_'))\n                s2_filename[2] = 's2'\n                s2_file = s2_path / '_'.join(s2_filename)\n\n                if not s2_file.exists():\n                    # print(f\"Missing optical image for SAR image: {s1_file.name} - {s2_file.name}\")\n                    continue\n\n                image_pairs.append((s1_file, s2_file))\n        \n        return image_pairs\n    \n    def _apply_predefined_split(self, split_file: Union[str, Path]) -> List[Tuple[Path, Path]]:\n        \"\"\"\n        Applies a predefined split from a JSON file.\n        \n        Args:\n            split_file: Path to JSON file containing split definitions\n            \n        Returns:\n            List[Tuple[Path, Path]]: Image pairs for the specified split\n        \"\"\"\n        try:\n            with open(split_file, 'r') as f:\n                splits = json.load(f)\n                \n            if self.split_type.value not in splits['data']:\n                raise ValueError(f\"Split type {self.split_type.value} not found in split file\")\n            \n            split_filenames = set(splits['data'][self.split_type.value]) # data['split']\n            return [pair for pair in self.all_image_pairs \n                if any(p.name in split_filenames for p in pair[:2])]\n        except Exception as e:\n            print(f'Could not open split file\\n\\t{e}')\n            raise\n\n    def _apply_random_split(\n        self, \n        split_ratio: Tuple[float, float, float],\n        seed: int\n    ) -> List[Tuple[Path, Path]]:\n        \"\"\"\n        Randomly splits the dataset according to the given ratios.\n        \n        Args:\n            split_ratio: Tuple of (train, val, test) ratios\n            seed: Random seed for reproducibility\n            \n        Returns:\n            List[Tuple[Path, Path]]: Image pairs for the specified split\n        \"\"\"\n        if sum(split_ratio) != 1:\n            raise ValueError(\"Split ratios must sum to 1\")\n        \n        # Set random seed for reproducibility\n        random.seed(seed)\n        \n        # Shuffle indices\n        indices = list(range(len(self.all_image_pairs)))\n        random.shuffle(indices)\n        \n        # Calculate split points\n        train_end = int(len(indices) * split_ratio[0])\n        val_end = train_end + int(len(indices) * split_ratio[1])\n        \n        # Select appropriate slice based on split type\n        if self.split_type == SplitType.TRAIN:\n            split_indices = indices[:train_end]\n        elif self.split_type == SplitType.VAL:\n            split_indices = indices[train_end:val_end]\n        else:  # TEST\n            split_indices = indices[val_end:]\n            \n        return [self.all_image_pairs[i] for i in split_indices]\n    \n    def save_split(self, output_file: Union[str, Path], append: bool = False):\n        \"\"\"\n        Saves the current split configuration to a JSON file.\n        \n        Args:\n            output_file: Path to save the split configuration\n            append: Define which mode you want to open the file in\n        \"\"\"\n        if self.split_type:\n            split = self.split_type.value\n            split_info = {\n                'data' : {\n                    split: [p[0].name for p in self.image_pairs]\n                }\n            }\n            mode = 'a' if append else 'w'\n            with open(output_file, mode) as f:\n                json.dump(split_info, f, indent=2)\n    \n    def __len__(self):\n        \"\"\"Returns the total number of image pairs in the dataset.\"\"\"\n        return len(self.image_pairs)\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Retrieves the image pair at the given index.\n        \n        Args:\n            idx (int): Index of the image pair to retrieve\n            \n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]: Processed (SAR image, optical image) pair\n        \"\"\"\n        # Get paths for SAR and optical images\n        s1_path, s2_path = self.image_pairs[idx]\n        \n        # Load images\n        s1_image = Image.open(s1_path).convert('RGB')\n        s2_image = Image.open(s2_path).convert('RGB')\n        \n        # Apply transforms\n        s1_image = self.transform(s1_image)\n        s2_image = self.transform(s2_image)\n        \n        return s1_image, s2_image","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:08:10.672302Z","iopub.execute_input":"2024-11-06T11:08:10.673120Z","iopub.status.idle":"2024-11-06T11:08:10.726894Z","shell.execute_reply.started":"2024-11-06T11:08:10.673058Z","shell.execute_reply":"2024-11-06T11:08:10.725451Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Building Blocks","metadata":{}},{"cell_type":"code","source":"class DownsamplingBlock(nn.Module):\n    \"\"\"Defines the Unet downsampling block. \n    \n    Consists of Convolution-BatchNorm-ReLU layer with k filters.\n    \"\"\"\n    def __init__(self, c_in, c_out, kernel_size=4, stride=2, \n                 padding=1, negative_slope=0.2, use_norm=True):\n        \"\"\"\n        Initializes the UnetDownsamplingBlock.\n        \n        Args:\n            c_in (int): The number of input channels.\n            c_out (int): The number of output channels.\n            kernel_size (int, optional): The size of the convolving kernel. Default is 4.\n            stride (int, optional): Stride of the convolution. Default is 2.\n            padding (int, optional): Zero-padding added to both sides of the input. Default is 0.\n            negative_slope (float, optional): Negative slope for the LeakyReLU activation function. Default is 0.2.\n            use_norm (bool, optinal): If use norm layer. If True add a BatchNorm layer after Conv. Default is True.\n        \"\"\"\n        super(DownsamplingBlock, self).__init__()\n        block = []\n        block += [nn.Conv2d(in_channels=c_in, out_channels=c_out,\n                          kernel_size=kernel_size, stride=stride, padding=padding,\n                          bias=(not use_norm) # No need to use a bias if there is a batchnorm layer after conv\n                          )]\n        if use_norm:\n            block += [nn.BatchNorm2d(num_features=c_out)]\n        \n        block += [nn.LeakyReLU(negative_slope=negative_slope)]\n\n        self.conv_block = nn.Sequential(*block)\n        \n    def forward(self, x):\n        return self.conv_block(x)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:08:16.726559Z","iopub.execute_input":"2024-11-06T11:08:16.727056Z","iopub.status.idle":"2024-11-06T11:08:16.739197Z","shell.execute_reply.started":"2024-11-06T11:08:16.727013Z","shell.execute_reply":"2024-11-06T11:08:16.737464Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class UpsamplingBlock(nn.Module):\n    \"\"\"Defines the Unet upsampling block.\n    \"\"\"\n    def __init__(self, c_in, c_out, kernel_size=4, stride=2, \n                 padding=1, use_dropout=False, use_upsampling=False, mode='nearest'):\n        \n        \"\"\"\n        Initializes the Unet Upsampling Block.\n        \n        Args:\n            c_in (int): The number of input channels.\n            c_out (int): The number of output channels.\n            kernel_size (int, optional): Size of the convolving kernel. Default is 4.\n            stride (int, optional): Stride of the convolution. Default is 2.\n            padding (int, optional): Zero-padding added to both sides of the input. Default is 0.\n            use_dropout (bool, optional): if use dropout layers. Default is False.\n            upsample (bool, optinal): if use upsampling rather than transpose convolution. Default is False.\n            mode (str, optional): the upsampling algorithm: one of 'nearest', \n                'bilinear', 'bicubic'. Default: 'nearest'\n        \"\"\"\n        super(UpsamplingBlock, self).__init__()\n        block = []\n        if use_upsampling:\n            # Transpose convolution causes checkerboard artifacts. Upsampling\n            # followed by a regular convolutions produces better results appearantly\n            # Please check for further reading: https://distill.pub/2016/deconv-checkerboard/\n            # Odena, et al., \"Deconvolution and Checkerboard Artifacts\", Distill, 2016. http://doi.org/10.23915/distill.00003\n            \n            mode = mode if mode in ('nearest', 'bilinear', 'bicubic') else 'nearest'\n            \n            block += [nn.Sequential(\n                nn.Upsample(scale_factor=2, mode=mode),\n                nn.Conv2d(in_channels=c_in, out_channels=c_out,\n                          kernel_size=3, stride=1, padding=padding,\n                          bias=False\n                          )\n                )]\n        else:\n            block += [nn.ConvTranspose2d(in_channels=c_in, \n                                         out_channels=c_out,\n                                         kernel_size=kernel_size, \n                                         stride=stride,\n                                         padding=padding, bias=False\n                                         )\n                     ]\n        \n        block += [nn.BatchNorm2d(num_features=c_out)]\n\n        if use_dropout:\n            block += [nn.Dropout(0.5)]\n            \n        block += [nn.ReLU()]\n\n        self.conv_block = nn.Sequential(*block)\n\n    def forward(self, x):\n        return self.conv_block(x)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:08:20.286297Z","iopub.execute_input":"2024-11-06T11:08:20.286844Z","iopub.status.idle":"2024-11-06T11:08:20.300127Z","shell.execute_reply.started":"2024-11-06T11:08:20.286784Z","shell.execute_reply":"2024-11-06T11:08:20.298725Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Networks","metadata":{}},{"cell_type":"markdown","source":"## Generator","metadata":{}},{"cell_type":"code","source":"class UnetEncoder(nn.Module):\n    \"\"\"Create the Unet Encoder Network.\n    \n    C64-C128-C256-C512-C512-C512-C512-C512\n    \"\"\"\n    def __init__(self, c_in=3, c_out=512):\n        \"\"\"\n        Constructs the Unet Encoder Network.\n\n        Ck denote a Convolution-BatchNorm-ReLU layer with k filters.\n            C64-C128-C256-C512-C512-C512-C512-C512\n        Args:\n            c_in (int, optional): Number of input channels.\n            c_out (int, optional): Number of output channels. Default is 512.\n        \"\"\"\n        super(UnetEncoder, self).__init__()\n        self.enc1 = DownsamplingBlock(c_in, 64, use_norm=False) # C64\n        self.enc2 = DownsamplingBlock(64, 128) # C128\n        self.enc3 = DownsamplingBlock(128, 256) # C256\n        self.enc4 = DownsamplingBlock(256, 512) # C512\n        self.enc5 = DownsamplingBlock(512, 512) # C512\n        self.enc6 = DownsamplingBlock(512, 512) # C512\n        self.enc7 = DownsamplingBlock(512, 512) # C512\n        self.enc8 = DownsamplingBlock(512, c_out) # C512\n\n    def forward(self, x):\n        x1 = self.enc1(x)\n        x2 = self.enc2(x1)\n        x3 = self.enc3(x2)\n        x4 = self.enc4(x3)\n        x5 = self.enc5(x4)\n        x6 = self.enc6(x5)\n        x7 = self.enc7(x6)\n        x8 = self.enc8(x7)\n        out = [x8, x7, x6, x5, x4, x3, x2, x1] # latest activation is the first element\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:08:23.241289Z","iopub.execute_input":"2024-11-06T11:08:23.241798Z","iopub.status.idle":"2024-11-06T11:08:23.255806Z","shell.execute_reply.started":"2024-11-06T11:08:23.241747Z","shell.execute_reply":"2024-11-06T11:08:23.254099Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class UnetDecoder(nn.Module):\n    \"\"\"Creates the Unet Decoder Network.\n    \"\"\"\n    def __init__(self, c_in=512, c_out=64, use_upsampling=False, mode='nearest'):\n        \"\"\"\n        Constructs the Unet Decoder Network.\n\n        Ck denote a Convolution-BatchNorm-ReLU layer with k filters.\n        \n        CDk denotes a Convolution-BatchNorm-Dropout-ReLU layer with a dropout rate of 50%.\n            CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n        Args:\n            c_in (int): Number of input channels.\n            c_out (int, optional): Number of output channels. Default is 512.\n            use_upsampling (bool, optional): Upsampling method for decoder. \n                If True, use upsampling layer followed regular convolution layer.\n                If False, use transpose convolution. Default is False\n            mode (str, optional): the upsampling algorithm: one of 'nearest', \n                'bilinear', 'bicubic'. Default: 'nearest'\n        \"\"\"\n        super(UnetDecoder, self).__init__()\n        self.dec1 = UpsamplingBlock(c_in, 512, use_dropout=True, use_upsampling=use_upsampling, mode=mode) # CD512\n        self.dec2 = UpsamplingBlock(1024, 512, use_dropout=True, use_upsampling=use_upsampling, mode=mode) # CD1024\n        self.dec3 = UpsamplingBlock(1024, 512, use_dropout=True, use_upsampling=use_upsampling, mode=mode) # CD1024\n        self.dec4 = UpsamplingBlock(1024, 512, use_upsampling=use_upsampling, mode=mode) # C1024\n        self.dec5 = UpsamplingBlock(1024, 256, use_upsampling=use_upsampling, mode=mode) # C1024\n        self.dec6 = UpsamplingBlock(512, 128, use_upsampling=use_upsampling, mode=mode) # C512\n        self.dec7 = UpsamplingBlock(256, 64, use_upsampling=use_upsampling, mode=mode) # C256\n        self.dec8 = UpsamplingBlock(128, c_out, use_upsampling=use_upsampling, mode=mode) # C128\n    \n\n    def forward(self, x):\n        x9 = torch.cat([x[1], self.dec1(x[0])], 1) # (N,1024,H,W)\n        x10 = torch.cat([x[2], self.dec2(x9)], 1) # (N,1024,H,W)\n        x11 = torch.cat([x[3], self.dec3(x10)], 1) # (N,1024,H,W)\n        x12 = torch.cat([x[4], self.dec4(x11)], 1) # (N,1024,H,W)\n        x13 = torch.cat([x[5], self.dec5(x12)], 1) # (N,512,H,W)\n        x14 = torch.cat([x[6], self.dec6(x13)], 1) # (N,256,H,W)\n        x15 = torch.cat([x[7], self.dec7(x14)], 1) # (N,128,H,W)\n        out = self.dec8(x15) # (N,64,H,W)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:08:25.693819Z","iopub.execute_input":"2024-11-06T11:08:25.694355Z","iopub.status.idle":"2024-11-06T11:08:25.711251Z","shell.execute_reply.started":"2024-11-06T11:08:25.694307Z","shell.execute_reply":"2024-11-06T11:08:25.709768Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class UnetGenerator(nn.Module):\n    \"\"\"Create a Unet-based generator\"\"\"\n    def __init__(self, c_in=3, c_out=3, use_upsampling=False, mode='nearest'):\n        \"\"\"\n        Constructs a Unet generator\n        Args:\n            c_in (int): The number of input channels.\n            c_out (int): The number of output channels.\n            use_upsampling (bool, optional): Upsampling method for decoder. \n                If True, use upsampling layer followed regular convolution layer.\n                If False, use transpose convolution. Default is False\n            mode (str, optional): the upsampling algorithm: one of 'nearest', \n                'bilinear', 'bicubic'. Default: 'nearest'\n        \"\"\"\n        super(UnetGenerator, self).__init__()\n        self.encoder = UnetEncoder(c_in=c_in)\n        self.decoder = UnetDecoder(use_upsampling=use_upsampling, mode=mode)\n        # In the paper, the authors state:\n        #   \"\"\"\n        #       After the last layer in the decoder, a convolution is applied\n        #       to map to the number of output channels (3 in general, except\n        #       in colorization, where it is 2), followed by a Tanh function.\n        #   \"\"\"\n        # However, in the official Lua implementation, only a Tanh layer is applied.\n        # Therefore, I took the liberty of adding a convolutional layer with a \n        # kernel size of 3.\n        # For more information please check the paper and official github repo:\n        # https://github.com/phillipi/pix2pix\n        # https://arxiv.org/abs/1611.07004\n        self.head = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=c_out,\n                      kernel_size=3, stride=1, padding=1,\n                      bias=True\n                      ), \n            nn.Tanh()\n            )\n    \n    def forward(self, x):\n        outE = self.encoder(x)\n        outD = self.decoder(outE)\n        out = self.head(outD)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:08:27.815384Z","iopub.execute_input":"2024-11-06T11:08:27.815830Z","iopub.status.idle":"2024-11-06T11:08:27.828034Z","shell.execute_reply.started":"2024-11-06T11:08:27.815789Z","shell.execute_reply":"2024-11-06T11:08:27.826474Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Discriminator","metadata":{}},{"cell_type":"code","source":"class PixelDiscriminator(nn.Module):\n    \"\"\"Create a PixelGAN discriminator (1x1 PatchGAN discriminator)\"\"\"\n    def __init__(self, c_in=3, c_hid=64):\n        \"\"\"Constructs a PixelGAN discriminator, a special form of PatchGAN Discriminator.\n        All convolutions are 1x1 spatial filters\n\n        Args:\n            c_in (int, optional): The number of input channels. Defaults to 3.\n            c_hid (int, optional): The number of channels after first conv layer.\n                Defaults to 64.\n        \"\"\"\n        super(PixelDiscriminator, self).__init__()\n        self.model = nn.Sequential(\n            DownsamplingBlock(c_in, c_hid, kernel_size=1, stride=1, padding=0, use_norm=False),\n            DownsamplingBlock(c_hid, c_hid*2, kernel_size=1, stride=1, padding=0),\n            nn.Conv2d(in_channels=c_hid*2, out_channels=1, kernel_size=1)\n            )\n        # Similar to PatchDiscriminator, there should be a sigmoid layer at the end of discriminator.\n        # However, nn.BCEWithLogitsLoss combines the sigmoid layer with BCE loss, \n        # providing greater numerical stability. Therefore, the discriminator outputs\n        # logits to take advantage of this stability.\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:08:30.279493Z","iopub.execute_input":"2024-11-06T11:08:30.279933Z","iopub.status.idle":"2024-11-06T11:08:30.290180Z","shell.execute_reply.started":"2024-11-06T11:08:30.279894Z","shell.execute_reply":"2024-11-06T11:08:30.288705Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class PatchDiscriminator(nn.Module):\n    \"\"\"Create a PatchGAN discriminator\"\"\"\n    def __init__(self, c_in=3, c_hid=64, n_layers=3):\n        \"\"\"Constructs a PatchGAN discriminator\n\n        Args:\n            c_in (int, optional): The number of input channels. Defaults to 3.\n            c_hid (int, optional): The number of channels after first conv layer.\n                Defaults to 64.\n            n_layers (int, optional): the number of convolution blocks in the \n                discriminator. Defaults to 3.\n        \"\"\"\n        super(PatchDiscriminator, self).__init__()\n        model = [DownsamplingBlock(c_in, c_hid, use_norm=False)]\n\n        n_p = 1  # multiplier for previous channel\n        n_c = 1  # multiplier for current channel\n        # last block is with stride of 1, therefore iterate (n_layers-1) times\n        for n in range(1, n_layers): \n            n_p = n_c\n            n_c = min(2**n, 8)  # The number of channels is 512 at most\n\n            model += [DownsamplingBlock(c_hid*n_p, c_hid*n_c)]\n        \n        n_p = n_c\n        n_c = min(2**n_layers, 8)\n        model += [DownsamplingBlock(c_hid*n_p, c_hid*n_c, stride=1)] # last block is with stride of 1\n\n        # last layer is a convolution followed by a Sigmoid function.\n        model += [nn.Conv2d(in_channels=c_hid*n_c, out_channels=1, \n                            kernel_size=4, stride=1, padding=1, bias=True\n                            )] \n        # Normally, there should be a sigmoid layer at the end of discriminator.\n        # However, nn.BCEWithLogitsLoss combines the sigmoid layer with BCE loss, \n        # providing greater numerical stability. Therefore, the discriminator outputs\n        # logits to take advantage of this stability.\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:08:33.151425Z","iopub.execute_input":"2024-11-06T11:08:33.151966Z","iopub.status.idle":"2024-11-06T11:08:33.164695Z","shell.execute_reply.started":"2024-11-06T11:08:33.151917Z","shell.execute_reply":"2024-11-06T11:08:33.163105Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class PatchGAN(nn.Module):\n    \"\"\"Create a PatchGAN discriminator\"\"\"\n    def __init__(self, c_in=3, c_hid=64, mode='patch', n_layers=3):\n        \"\"\"Constructs a PatchGAN discriminator.\n\n        Args:\n            c_in (int, optional): The number of input channels. Defaults to 3.\n            c_hid (int, optional): The number of channels after first \n                convolutional layer. Defaults to 64.\n            mode (str, optional): PatchGAN type. Use 'pixel' for PixelGAN, and \n                'patch' for other types. Defaults to 'patch'.\n            n_layers (int, optional): PatchGAN number of layers. Defaults to 3.\n                - 16x16 PatchGAN if n=1\n                - 34x34 PatchGAN if n=2\n                - 70x70 PatchGAN if n=3\n                - 142x142 PatchGAN if n=4\n                - 286x286 PatchGAN if n=5\n                - 574x574 PatchGAN if n=6\n        \"\"\"\n        super(PatchGAN, self).__init__()\n        if mode == 'pixel':\n            self.model = PixelDiscriminator(c_in, c_hid)\n        else:\n            self.model = PatchDiscriminator(c_in, c_hid, n_layers)\n    \n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:08:36.010020Z","iopub.execute_input":"2024-11-06T11:08:36.011330Z","iopub.status.idle":"2024-11-06T11:08:36.022107Z","shell.execute_reply.started":"2024-11-06T11:08:36.011265Z","shell.execute_reply":"2024-11-06T11:08:36.020561Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Pix2Pix","metadata":{}},{"cell_type":"code","source":"class Pix2Pix(nn.Module):\n    \"\"\"Create a Pix2Pix class. It is a model for image to image translation tasks.\n    By default, the model uses a Unet architecture for generator with transposed\n    convolution. The discriminator is 70x70 PatchGAN discriminator, by default.\n     \"\"\"\n    def __init__(self, \n                 c_in: int = 3, \n                 c_out: int = 3, \n                 is_train: bool = True,\n                 netD: str = 'patch',\n                 lambda_L1: float = 100.0,\n                 is_CGAN: bool = True,\n                 use_upsampling: bool = False,\n                 mode: str = 'nearest',\n                 c_hid: int = 64,\n                 n_layers: int = 3,\n                 lr: float = 0.0002,\n                 beta1: float = 0.5,\n                 beta2: float = 0.999,\n                 ):\n        \"\"\"Constructs the Pix2Pix class.\n        \n        Args:\n            c_in: Number of input channels\n            c_out: Number of output channels\n            is_train: Whether the model is in training mode\n            netD: Type of discriminator ('patch' or 'pixel')\n            lambda_L1: Weight for L1 loss\n            is_CGAN: If True, use conditional GAN architecture\n            use_upsampling: If True, use upsampling in generator instead of transpose conv\n            mode: Upsampling mode ('nearest', 'bilinear', 'bicubic')\n            c_hid: Number of base filters in discriminator\n            n_layers: Number of layers in discriminator\n            lr: Learning rate\n            beta1: Beta1 parameter for Adam optimizer\n            beta2: Beta2 parameter for Adam optimizer\n        \"\"\"\n        super(Pix2Pix, self).__init__()\n        self.is_CGAN = is_CGAN\n        self.lambda_L1 = lambda_L1\n\n        self.gen = UnetGenerator(c_in=c_in, c_out=c_out, use_upsampling=use_upsampling, mode=mode)\n        self.gen = self.gen.apply(self.weights_init)\n        \n        if is_train:\n            # Conditional GANs need both input and output together, the total input channel is c_in+c_out\n            disc_in = c_in + c_out if is_CGAN else c_out\n            self.disc = PatchGAN(c_in=disc_in, c_hid=c_hid, mode=netD, n_layers=n_layers) \n            self.disc = self.disc.apply(self.weights_init)\n\n            # Initialize optimizers\n            self.gen_optimizer = torch.optim.Adam(\n                self.gen.parameters(), lr=lr, betas=(beta1, beta2))\n            self.disc_optimizer = torch.optim.Adam(\n                self.disc.parameters(), lr=lr, betas=(beta1, beta2))\n\n            # Initialize loss functions\n            self.criterion = nn.BCEWithLogitsLoss()\n            self.criterion_L1 = nn.L1Loss()\n    \n    def forward(self, x):\n        return self.gen(x)\n    \n    @staticmethod    \n    def weights_init(m):\n        \"\"\"Initialize network weights.\n        \n        Args:\n            m: network module\n        \"\"\"\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n            nn.init.normal_(m.weight, 0.0, 0.02)\n            if hasattr(m, 'bias') and m.bias is not None:\n                nn.init.constant_(m.bias, 0.0)\n        if isinstance(m, nn.BatchNorm2d):\n            nn.init.normal_(m.weight, 1.0, 0.02)\n            nn.init.constant_(m.bias, 0)\n\n    def _get_disc_inputs(self, real_images, target_images, fake_images):\n        \"\"\"Prepare discriminator inputs based on conditional/unconditional setup.\"\"\"\n        if self.is_CGAN:\n            # Conditional GANs need both input and output together, \n            # Therefore, the total input channel is c_in+c_out\n            real_AB = torch.cat([real_images, target_images], dim=1)\n            fake_AB = torch.cat([real_images, \n                               fake_images.detach()], \n                               dim=1)\n        else:\n            real_AB = target_images\n            fake_AB = fake_images.detach()\n        return real_AB, fake_AB\n    \n    def _get_gen_inputs(self, real_images, fake_images):\n        \"\"\"Prepare discriminator inputs based on conditional/unconditional setup.\"\"\"\n        if self.is_CGAN:\n            # Conditional GANs need both input and output together, \n            # Therefore, the total input channel is c_in+c_out\n            fake_AB = torch.cat([real_images, \n                               fake_images], \n                               dim=1)\n        else:\n            fake_AB = fake_images\n        return fake_AB\n    \n    \n    def step_discriminator(self, real_images, target_images, fake_images):\n        \"\"\"Discriminator forward/backward pass.\n        \n        Args:\n            real_images: Input images\n            target_images: Ground truth images\n            fake_images: Generated images\n            \n        Returns:\n            Discriminator loss value\n        \"\"\"\n        # Prepare inputs\n        real_AB, fake_AB = self._get_disc_inputs(real_images, target_images, \n                                                fake_images)\n          \n        # Forward pass through the discriminator\n        pred_real = self.disc(real_AB) # D(x, y)\n        pred_fake = self.disc(fake_AB) # D(x, G(x))\n\n        # Compute the losses\n        lossD_real = self.criterion(pred_real, torch.ones_like(pred_real)) # (D(x, y), 1)\n        lossD_fake = self.criterion(pred_fake, torch.zeros_like(pred_fake)) # (D(x, y), 0)\n        lossD = (lossD_real + lossD_fake) * 0.5 # Combined Loss\n        return lossD\n    \n    def step_generator(self, real_images, target_images, fake_images):\n        \"\"\"Discriminator forward/backward pass.\n        \n        Args:\n            real_images: Input images\n            target_images: Ground truth images\n            fake_images: Generated images\n            \n        Returns:\n            Discriminator loss value\n        \"\"\"\n        # Prepare input\n        fake_AB = self._get_gen_inputs(real_images, fake_images)\n          \n        # Forward pass through the discriminator\n        pred_fake = self.disc(fake_AB)\n\n        # Compute the losses\n        lossG_GaN = self.criterion(pred_fake, torch.ones_like(pred_fake)) # GAN Loss\n        lossG_L1 = self.criterion_L1(fake_images, target_images)           # L1 Loss\n        lossG = lossG_GaN + self.lambda_L1 * lossG_L1                      # Combined Loss\n        # Return total loss and individual components\n        return lossG, {\n            'loss_G': lossG.item(),\n            'loss_G_GAN': lossG_GaN.item(),\n            'loss_G_L1': lossG_L1.item()\n        }\n    \n    def train_step(self, real_images, target_images):\n        \"\"\"Performs a single training step.\n        \n        Args:\n            real_images: Input images\n            target_images: Ground truth images\n            \n        Returns:\n            Dictionary containing all loss values from this step\n        \"\"\"\n        # Forward pass through the generator\n        fake_images = self.forward(real_images)\n        \n        # Update discriminator\n        self.disc_optimizer.zero_grad() # Reset the gradients for D\n        lossD = self.step_discriminator(real_images, target_images, fake_images) # Compute the loss\n        lossD.backward()\n        self.disc_optimizer.step() # Update D\n\n        # Update generator\n        self.gen_optimizer.zero_grad() # Reset the gradients for D\n        lossG, G_losses = self.step_generator(real_images, target_images, fake_images) # Compute the loss\n        lossG.backward()\n        self.gen_optimizer.step() # Update D\n\n        # Return all losses\n        return {\n            'loss_D': lossD.item(),\n            **G_losses\n        }\n    \n    def get_current_visuals(self, real_images, target_images):\n        \"\"\"Return visualization images.\n        \n        Args:\n            real_images: Input images\n            target_images: Ground truth images\n            \n        Returns:\n            Dictionary containing input, target and generated images\n        \"\"\"\n        with torch.no_grad():\n            fake_images = self.gen(real_images)\n        return {\n            'real': real_images,\n            'fake': fake_images,\n            'target': target_images\n        }","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:08:38.565608Z","iopub.execute_input":"2024-11-06T11:08:38.566076Z","iopub.status.idle":"2024-11-06T11:08:38.601109Z","shell.execute_reply.started":"2024-11-06T11:08:38.566036Z","shell.execute_reply":"2024-11-06T11:08:38.599561Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"PARAMS = {\n    'netD' : 'patch',\n    'lambda_L1' : 100.0,\n    'is_CGAN' : True,\n    'use_upsampling' : False,\n    'mode' : 'nearest',\n    'c_hid' : 64,\n    'n_layers' : 3,\n    'lr' : 0.0002,\n    'beta1' : 0.5,\n    'beta2' : 0.999,\n    'batch_size' : 32,\n    'epochs' : 70,\n    'seed' : 42\n    }\n\nSEED = PARAMS['seed']\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\ntorch.manual_seed(SEED)\n\nexperiment.log_parameters(PARAMS)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:08:51.954629Z","iopub.execute_input":"2024-11-06T11:08:51.955199Z","iopub.status.idle":"2024-11-06T11:08:51.976138Z","shell.execute_reply.started":"2024-11-06T11:08:51.955120Z","shell.execute_reply":"2024-11-06T11:08:51.974482Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7f1de4258350>"},"metadata":{}}]},{"cell_type":"code","source":"# Initialize the Pix2Pix model\nmodel = Pix2Pix(\n    is_train=True,\n    netD=PARAMS['netD'],\n    lambda_L1=PARAMS['lambda_L1'],\n    is_CGAN=PARAMS['is_CGAN'],\n    use_upsampling=PARAMS['use_upsampling'],\n    mode=PARAMS['mode'],\n    c_hid=PARAMS['c_hid'],\n    n_layers=PARAMS['n_layers'],\n    lr=PARAMS['lr'],\n    beta1=PARAMS['beta1'],\n    beta2=PARAMS['beta2']\n    )\n\ntotal_params = sum(p.numel() for p in model.gen.parameters())\ntotal_trainable_params = sum(p.numel() for p in model.gen.parameters() if p.requires_grad)\nprint('Generator:')\nprint(f\"Total params: {total_params}, Total trainable params: {total_trainable_params}\")\n\ntotal_params = sum(p.numel() for p in model.disc.parameters())\ntotal_trainable_params = sum(p.numel() for p in model.disc.parameters() if p.requires_grad)\nprint('Discriminator:')\nprint(f\"Total params: {total_params}, Total trainable params: {total_trainable_params}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:08:56.918562Z","iopub.execute_input":"2024-11-06T11:08:56.919086Z","iopub.status.idle":"2024-11-06T11:08:58.131201Z","shell.execute_reply.started":"2024-11-06T11:08:56.919037Z","shell.execute_reply":"2024-11-06T11:08:58.129835Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Generator:\nTotal params: 54541827, Total trainable params: 54541827\nDiscriminator:\nTotal params: 2768705, Total trainable params: 2768705\n","output_type":"stream"}]},{"cell_type":"code","source":"model.to(DEVICE)\nmodel = torch.compile(model)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:09:00.557518Z","iopub.execute_input":"2024-11-06T11:09:00.558844Z","iopub.status.idle":"2024-11-06T11:09:03.087091Z","shell.execute_reply.started":"2024-11-06T11:09:00.558786Z","shell.execute_reply":"2024-11-06T11:09:03.085335Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Root Path\nroot_dir = '/kaggle/input/sentinel12-image-pairs-segregated-by-terrain/v_2'\nsplit_save_path = '/kaggle/working/split.json'\n# Load the custom dataset\ntrain_transforms = v2.Compose([\n    v2.ToImage(),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize(mean=[0.5], std=[0.5]),\n])\n\ndataset = Sentinel(\n    root_dir=root_dir, \n    split_type='train', \n    transform=train_transforms, \n    split_mode='random',\n    split_ratio=(0.8,0.1,0.1),\n    seed=SEED\n    )\ndataset.save_split(output_file=split_save_path)\n\ndataloader = DataLoader(\n    dataset, \n    batch_size=PARAMS['batch_size'], \n    shuffle=True, \n    num_workers=4\n    )","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:09:13.252361Z","iopub.execute_input":"2024-11-06T11:09:13.253012Z","iopub.status.idle":"2024-11-06T11:09:59.686252Z","shell.execute_reply.started":"2024-11-06T11:09:13.252966Z","shell.execute_reply":"2024-11-06T11:09:59.684566Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Total image pairs found: 12800\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train the model\nnum_epochs = PARAMS['epochs']\nlen_batch = len(dataloader)\nsave_freq = 5\nbase_path = '/kaggle/working/'\nfor epoch in range(1, num_epochs+1):\n    total_lossD = 0.0\n    total_lossG = 0.0\n    total_lossG_GAN = 0.0\n    total_lossG_L1 = 0.0\n    for real_images, target_images in dataloader:\n        real_images, target_images = real_images.to(DEVICE), target_images.to(DEVICE)\n        losses = model.train_step(real_images, target_images)\n        total_lossD += losses['loss_D']\n        total_lossG += losses['loss_G']\n        total_lossG_GAN += losses['loss_G_GAN']\n        total_lossG_L1 += losses['loss_G_L1']\n        \n    loss_D = total_lossD / len_batch\n    loss_G = total_lossG / len_batch\n    loss_G_GAN = total_lossG_GAN / len_batch\n    loss_G_L1 = total_lossG_L1 / len_batch\n    # Log the losses\n    print(f\"Epoch [{epoch}/{num_epochs}] - Loss_D: {loss_D:.4f}, Loss_G: {loss_G:.4f}\")\n    experiment.log_metrics(\n        {\n            'epoch': epoch, \n            'lossD': loss_D,\n            'lossG' : loss_G,\n            'lossG_GAN' : loss_G_GAN,\n            'lossG_L1' : loss_G_L1\n        }, \n        step=epoch\n        )\n    \n    if epoch % save_freq == 0:\n        ckpt_path = base_path + f'pix2pix_gen_ckpt_{epoch+1}.pt'\n        torch.save(model.gen.state_dict(), ckpt_path)\n        ckpt_path = base_path + f'pix2pix_disc_ckpt_{epoch+1}.pt'\n        torch.save(model.disc.state_dict(), ckpt_path)\n    \n        \nexperiment.end()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T11:10:36.196612Z","iopub.execute_input":"2024-11-06T11:10:36.197108Z","iopub.status.idle":"2024-11-06T11:11:32.603437Z","shell.execute_reply.started":"2024-11-06T11:10:36.197063Z","shell.execute_reply":"2024-11-06T11:11:32.601614Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch [1/70] - Loss_D: 0.0023, Loss_G: 0.1096\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m         ckpt_path \u001b[38;5;241m=\u001b[39m base_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpix2pix_disc_ckpt_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     41\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mdisc\u001b[38;5;241m.\u001b[39mstate_dict(), ckpt_path)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241m.\u001b[39mend()\n","\u001b[0;31mNameError\u001b[0m: name 'experiment' is not defined"],"ename":"NameError","evalue":"name 'experiment' is not defined","output_type":"error"}]}]}