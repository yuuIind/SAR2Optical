{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QABfw8cbVQt0"
   },
   "source": [
    "# Pix2Pix Image Translation: SAR to RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNOeYytaVQt3"
   },
   "source": [
    "This is a Pix2Pix CGAN implementation for translating Synthetic Aperture Radar (SAR) images to optical images. Throughout the notebook, we first implement the pix2pix architecture from building blocks to the whole architecture step by step. If you are not familiar with the architecture or want to brush up on the concepts, you can consider reading these papers:\n",
    "1. Generative Adversarial Networks (GANs): [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661), (Goodfellow, et al. 2014)\n",
    "2. U-net: [U-net: Convolutional networks for biomedical image segmentation](https://arxiv.org/abs/1505.04597), (Ronneberger, et al. 2015)\n",
    "3. Pix2Pix: [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004), (Isola, et al. 2017)\n",
    "\n",
    "The Pix2Pix model is an architecture designed for image-to-image translation tasks, such as converting sketches to photos or colorizing black-and-white images. The Pix2Pix architecture uses two main components:\n",
    "\n",
    "1. Generator: The generator network creates fake images from input data, using an U-Net like architecture.\n",
    "\n",
    "2. Discriminator: The discriminator distinguishes between real and fake images. The goal of the discriminator is to classify whether a given image is real or fake. In Pix2Pix, a 70x70 PatchGAN is often used, where the discriminator processes image patches rather than the full image.\n",
    "\n",
    "The model is trained using adversarial loss and a reconstruction loss (L1 loss) to encourage the generator to produce high-quality images that are close to the ground truth.\n",
    "\n",
    "I will use PyTorch for implementing the model and the Sentinel SAR & Optical Image Pairs dataset. The dataset is available on [Kaggle](https://www.kaggle.com/) at [Sentinel-1&2 Image Pairs (SAR & Optical)](https://www.kaggle.com/datasets/requiemonk/sentinel12-image-pairs-segregated-by-terrain), uploaded and curated by [Paritosh Tiwari (@requiemonk)](https://www.kaggle.com/requiemonk).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Un5kJHwVQt6"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqLvCjIpVQt8"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "from typing import Tuple, List, Optional, Callable, Union, Literal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKCCYgFFVQt_"
   },
   "source": [
    "## Prepare The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgNKKV06VQt_"
   },
   "source": [
    "Here is a custom PyTorch dataset class. We will use it later when we train our model. This class helps us manage the dataset conveniently. Technically, it's not always necessary to write your own dataset classes, but I prefer to do so because it provides more flexibility and control. Writing a custom dataset class is quite simple: Your class should inherit from the ``Dataset`` class and override the following two methods:\n",
    "    1. `__len__`: This method returns the size of the dataset when you call `len(dataset)`\n",
    "    2. `__getitem__`: his method allows indexing, so `dataset[i]` will return the i-th example.\n",
    "\n",
    "In our implementation, we’ve added extra methods to assist with finding pairs, splitting the data, and creating and saving data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYxxbvYsVQuA"
   },
   "outputs": [],
   "source": [
    "class SplitType(Enum):\n",
    "    \"\"\"Enumeration for dataset split types\"\"\"\n",
    "    TRAIN = 'train'\n",
    "    VAL = 'val'\n",
    "    TEST = 'test'\n",
    "\n",
    "\n",
    "class Sentinel(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for handling Sentinel-1&2 Image Pairs.\n",
    "\n",
    "    This dataset assumes a directory structure of:\n",
    "    root_dir/\n",
    "        category1/\n",
    "            s1/\n",
    "                image1.png\n",
    "                image2.png\n",
    "            s2/\n",
    "                image1.png\n",
    "                image2.png\n",
    "        category2/\n",
    "            ...\n",
    "\n",
    "    This class has support for train/val/test splits. When `split_type` is `None`,\n",
    "    uses the complete dataset. When `split_type` is specified\n",
    "    (``'train'``, ``'val'``, ``'test'``), the dataset can be split using:\n",
    "\n",
    "    1. A split that defines which images belong to which split\n",
    "    2. Random splitting with a specified ratio\n",
    "\n",
    "    Args:\n",
    "        root_dir (str | Path): Root directory containing the dataset\n",
    "        split_type (str | None): Which split to use ('train', 'val', 'test') or None for full dataset\n",
    "        transform (callable, optional): Transform to apply to both SAR and optical images\n",
    "        split_mode (str, optional): How to split the dataset ('random', 'split')\n",
    "        split_ratio (Tuple[float, float, float], optional): Ratio for train/val/test splits\n",
    "        split_file (str | Path, optional): predefined the splits\n",
    "        seed (int, optional): Random seed for reproducible splitting\n",
    "\n",
    "    Attributes:\n",
    "        root_dir (Path): Path to the dataset root directory\n",
    "        transform (callable): Transform pipeline for the images\n",
    "        image_pairs (List[Tuple[Path, Path]]): List of paired image paths (SAR, optical)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root_dir: Union[str, Path],\n",
    "                 split_type: Optional[str] = None,\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 split_mode: Literal['random', 'split'] = 'random',\n",
    "                 split_ratio: Tuple[float, float, float] = (0.7, 0.15, 0.15),\n",
    "                 split_file: Optional[Union[str, Path]] = None,\n",
    "                 seed: int = 42):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        if not self.root_dir.exists():\n",
    "            raise FileNotFoundError(f\"Dataset root directory not found: {self.root_dir}\")\n",
    "\n",
    "        # Convert string split_type to enum if provided\n",
    "        self.split_type = SplitType(split_type) if split_type else None\n",
    "\n",
    "        # Default transform pipeline\n",
    "        self.transform = transform if transform else v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True)\n",
    "        ])\n",
    "\n",
    "        # Collect image pairs\n",
    "        self.all_image_pairs = self._collect_images()\n",
    "\n",
    "        # Apply split if specified\n",
    "        if split_type:\n",
    "            if split_mode == 'split' and split_file:\n",
    "                self.image_pairs = self._apply_predefined_split(split_file)\n",
    "            elif split_mode == 'random':\n",
    "                self.image_pairs = self._apply_random_split(split_ratio, seed)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid split configuration. Use either 'split' with a split_file or 'random' with split_ratio\")\n",
    "        else:\n",
    "            # If no split type specified, use all images\n",
    "            self.image_pairs = self.all_image_pairs\n",
    "\n",
    "        print(f'Total image pairs found: {len(self)}')\n",
    "\n",
    "    def _collect_images(self) -> List[Tuple[Path, Path]]:\n",
    "        \"\"\"\n",
    "        Collects paired SAR (s1) and optical (s2) image paths from the dataset directory.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[Path, Path]]: List of (SAR image path, optical image path) pairs\n",
    "        \"\"\"\n",
    "        image_pairs = []\n",
    "\n",
    "        # Iterate through category subdirectories\n",
    "        for category in self.root_dir.iterdir():\n",
    "            # Check if it's a directory\n",
    "            if not category.is_dir():\n",
    "                continue\n",
    "\n",
    "            s1_path = category / 's1'\n",
    "            s2_path = category / 's2'\n",
    "\n",
    "            if not (s1_path.is_dir() and s2_path.is_dir()):\n",
    "                # print(f\"Missing s1 or s2 subdirectory in category: {category.name}\")\n",
    "                continue\n",
    "\n",
    "            # Collect pairs\n",
    "            for s1_file in s1_path.glob('*.png'):\n",
    "                # Convert SAR filename to optical filename\n",
    "                # e.g. 'ROIs1970_fall_s1_13_p265.png' -> 'ROIs1970_fall_s2_13_p265.png'\n",
    "                s2_filename = list(s1_file.name.split('_'))\n",
    "                s2_filename[2] = 's2'\n",
    "                s2_file = s2_path / '_'.join(s2_filename)\n",
    "\n",
    "                if not s2_file.exists():\n",
    "                    # print(f\"Missing optical image for SAR image: {s1_file.name} - {s2_file.name}\")\n",
    "                    continue\n",
    "\n",
    "                image_pairs.append((s1_file, s2_file))\n",
    "\n",
    "        return image_pairs\n",
    "\n",
    "    def _apply_predefined_split(self, split_file: Union[str, Path]) -> List[Tuple[Path, Path]]:\n",
    "        \"\"\"\n",
    "        Applies a predefined split from a JSON file.\n",
    "\n",
    "        Args:\n",
    "            split_file: Path to JSON file containing split definitions\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[Path, Path]]: Image pairs for the specified split\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(split_file, 'r') as f: # get the split content\n",
    "                splits = json.load(f)\n",
    "\n",
    "            if self.split_type.value not in splits['data']: # check if it helds\n",
    "                raise ValueError(f\"Split type {self.split_type.value} not found in split file\")\n",
    "\n",
    "            split_filenames = set(splits['data'][self.split_type.value]) # data['split']\n",
    "            return [pair for pair in self.all_image_pairs # collect and return split\n",
    "                if any(str(p.relative_to(self.root_dir)) in split_filenames for p in pair[:2])]\n",
    "        except Exception as e:\n",
    "            print(f'Could not open split file\\n\\t{e}')\n",
    "            raise\n",
    "\n",
    "    def _apply_random_split(\n",
    "        self,\n",
    "        split_ratio: Tuple[float, float, float],\n",
    "        seed: int\n",
    "    ) -> List[Tuple[Path, Path]]:\n",
    "        \"\"\"\n",
    "        Randomly splits the dataset according to the given ratios.\n",
    "\n",
    "        Args:\n",
    "            split_ratio: Tuple of (train, val, test) ratios\n",
    "            seed: Random seed for reproducibility\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[Path, Path]]: Image pairs for the specified split\n",
    "        \"\"\"\n",
    "        if sum(split_ratio) != 1:\n",
    "            raise ValueError(\"Split ratios must sum to 1\")\n",
    "\n",
    "        # Set random seed for reproducibility\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Shuffle indices\n",
    "        indices = list(range(len(self.all_image_pairs)))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        # Calculate split points\n",
    "        train_end = int(len(indices) * split_ratio[0])\n",
    "        val_end = train_end + int(len(indices) * split_ratio[1])\n",
    "\n",
    "        # Select appropriate slice based on split type\n",
    "        if self.split_type == SplitType.TRAIN:\n",
    "            split_indices = indices[:train_end]\n",
    "        elif self.split_type == SplitType.VAL:\n",
    "            split_indices = indices[train_end:val_end]\n",
    "        else:  # TEST\n",
    "            split_indices = indices[val_end:]\n",
    "\n",
    "        return [self.all_image_pairs[i] for i in split_indices]\n",
    "\n",
    "    def save_split(self, output_file: Union[str, Path], is_exists: bool = False):\n",
    "        \"\"\"\n",
    "        Saves the current split configuration to a JSON file.\n",
    "\n",
    "        Args:\n",
    "            output_file: Path to save the split configuration\n",
    "            is_exists: If file exist, add new split data\n",
    "        \"\"\"\n",
    "        if self.split_type:\n",
    "            split = self.split_type.value\n",
    "            split_info = {\n",
    "                'data' : {\n",
    "                    split: [str(p[0].relative_to(self.root_dir)) for p in self.image_pairs]\n",
    "                }\n",
    "            }\n",
    "            # Check if the file exists\n",
    "            if is_exists and Path(output_file).exists():\n",
    "                # Read the existing content\n",
    "                with open(output_file, 'r') as f:\n",
    "                    existing_data = json.load(f)\n",
    "                # Check if 'data' is already in the existing content, if not, create it\n",
    "                if 'data' not in existing_data:\n",
    "                    existing_data['data'] = {}\n",
    "\n",
    "                # Add or update the split information\n",
    "                existing_data['data'][split] = split_info['data'][split]\n",
    "                split_info = existing_data\n",
    "\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(split_info, f, indent=2)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of image pairs in the dataset.\"\"\"\n",
    "        return len(self.image_pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieves the image pair at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the image pair to retrieve\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Processed (SAR image, optical image) pair\n",
    "        \"\"\"\n",
    "        # Get paths for SAR and optical images\n",
    "        s1_path, s2_path = self.image_pairs[idx]\n",
    "\n",
    "        # Load images\n",
    "        s1_image = Image.open(s1_path).convert('RGB')\n",
    "        s2_image = Image.open(s2_path).convert('RGB')\n",
    "\n",
    "        # Apply transforms\n",
    "        s1_image = self.transform(s1_image)\n",
    "        s2_image = self.transform(s2_image)\n",
    "\n",
    "        return s1_image, s2_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wonoYmX9VQuE"
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCCgVun_VQuE"
   },
   "source": [
    "Now, we can start with our implementation. First, we will implement our building blocks. Then, we’ll move on to implementing the generator and discriminator networks. Finally, we will combine them to form the complete architecture. Let’s get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe5xhNx7VQuH"
   },
   "source": [
    "### Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSYgq2A2VQuH"
   },
   "source": [
    "The **Pix2Pix** architecture consists of several layers arranged in a specific order, which are then repeatedly used throughout the network. By implementing these layers as smaller, modular blocks, we avoid redundant code and achieve a more flexible, modular design. This modularity allows us to easily adjust the configuration of the architecture.\n",
    "\n",
    "There are two main types of blocks in the pix2pix architecture: the standard convolutional blocks and the upsampling blocks. The convolutional blocks are the usual blocks found in many convolutional networks, while the upsampling blocks use transpose convolution to increase the spatial dimensions, rather than reducing them.\n",
    "\n",
    "- **Convolutional blocks** consist of a Convolution Layer, a BatchNorm Layer, and a ReLU layer, in that order.\n",
    "- **Upsampling blocks** are similar but instead consist of a Transpose Convolution Layer, a BatchNorm Layer, a Dropout Layer, and a ReLU layer in that order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFCfek4fVQuI"
   },
   "source": [
    "For convolutional blocks, if padding is not used, the spatial dimensions usually decrease. Therefore, `DownsamplingBlock` is appropriate in my opinion, even when padding is used to keep the spatial resolution the same. This naming convention also works well with upsampling blocks.\n",
    "\n",
    "\n",
    "The module itself is straightforward:\n",
    "1. Convolution Layer\n",
    "2. BatchNorm\n",
    "3. ReLU\n",
    "\n",
    "In the pix2pix architecture, all downsampling blocks use the Leaky ReLU activation function with a slope of 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKCzl5oeVQuI"
   },
   "outputs": [],
   "source": [
    "class DownsamplingBlock(nn.Module):\n",
    "    \"\"\"Defines the Unet downsampling block.\n",
    "\n",
    "    Consists of Convolution-BatchNorm-ReLU layer with k filters.\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in, c_out, kernel_size=4, stride=2,\n",
    "                 padding=1, negative_slope=0.2, use_norm=True):\n",
    "        \"\"\"\n",
    "        Initializes the UnetDownsamplingBlock.\n",
    "\n",
    "        Args:\n",
    "            c_in (int): The number of input channels.\n",
    "            c_out (int): The number of output channels.\n",
    "            kernel_size (int, optional): The size of the convolving kernel. Default is 4.\n",
    "            stride (int, optional): Stride of the convolution. Default is 2.\n",
    "            padding (int, optional): Zero-padding added to both sides of the input. Default is 0.\n",
    "            negative_slope (float, optional): Negative slope for the LeakyReLU activation function. Default is 0.2.\n",
    "            use_norm (bool, optinal): If use norm layer. If True add a BatchNorm layer after Conv. Default is True.\n",
    "        \"\"\"\n",
    "        super(DownsamplingBlock, self).__init__()\n",
    "        block = []\n",
    "        block += [nn.Conv2d(in_channels=c_in, out_channels=c_out,\n",
    "                          kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                          bias=(not use_norm) # No need to use a bias if there is a batchnorm layer after conv\n",
    "                          )]\n",
    "        if use_norm:\n",
    "            block += [nn.BatchNorm2d(num_features=c_out)]\n",
    "\n",
    "        block += [nn.LeakyReLU(negative_slope=negative_slope)]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdUi72bqVQuJ"
   },
   "source": [
    "The **upsampling block** is similar to the **downsampling block**, but it differs in three ways:\n",
    "1. It may include a dropout layer between the normalization layer and the activation function.\n",
    "2. The activation function is **ReLU**.\n",
    "3. Instead of a normal convolution, it uses **transpose convolution**.\n",
    "\n",
    "In the pix2pix architecture, some upsampling blocks use the dropout layer. To accommodate this, our implementation supports dropout.\n",
    "\n",
    "One important thing to note is that the upsampling block can use an upsample operation followed by a convolution layer, instead of transpose convolution. While the original architecture uses transpose convolutions, I’ve added the option to use the classic upsampling operation followed by a normal convolution. It turns out that transpose can sometimes cause checkerboard-like artifacts in the output. This change addresses this issue with transpose convolutions. For our dataset, transpose convolutions perform well, so this adjustment is not strictly necessary. However, it's useful to be aware of this option.\n",
    "\n",
    "For more details, I recommend checking out this lovely blog post from the Google Brain team:  \n",
    "[Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/) at [distill.pub](https://distill.pub/)\n",
    "\n",
    "So, the upsampling block consists of:\n",
    "1. Transpose Convolution Layer\n",
    "2. BatchNorm\n",
    "3. Dropout (optional)\n",
    "4. ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GpgGkgUVQuJ"
   },
   "outputs": [],
   "source": [
    "class UpsamplingBlock(nn.Module):\n",
    "    \"\"\"Defines the Unet upsampling block.\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in, c_out, kernel_size=4, stride=2,\n",
    "                 padding=1, use_dropout=False, use_upsampling=False, mode='nearest'):\n",
    "\n",
    "        \"\"\"\n",
    "        Initializes the Unet Upsampling Block.\n",
    "\n",
    "        Args:\n",
    "            c_in (int): The number of input channels.\n",
    "            c_out (int): The number of output channels.\n",
    "            kernel_size (int, optional): Size of the convolving kernel. Default is 4.\n",
    "            stride (int, optional): Stride of the convolution. Default is 2.\n",
    "            padding (int, optional): Zero-padding added to both sides of the input. Default is 0.\n",
    "            use_dropout (bool, optional): if use dropout layers. Default is False.\n",
    "            upsample (bool, optinal): if use upsampling rather than transpose convolution. Default is False.\n",
    "            mode (str, optional): the upsampling algorithm: one of 'nearest',\n",
    "                'bilinear', 'bicubic'. Default: 'nearest'\n",
    "        \"\"\"\n",
    "        super(UpsamplingBlock, self).__init__()\n",
    "        block = []\n",
    "        if use_upsampling:\n",
    "            # Transpose convolution causes checkerboard artifacts. Upsampling\n",
    "            # followed by a regular convolutions produces better results appearantly\n",
    "            # Please check for further reading: https://distill.pub/2016/deconv-checkerboard/\n",
    "            # Odena, et al., \"Deconvolution and Checkerboard Artifacts\", Distill, 2016. http://doi.org/10.23915/distill.00003\n",
    "\n",
    "            mode = mode if mode in ('nearest', 'bilinear', 'bicubic') else 'nearest'\n",
    "\n",
    "            block += [nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode=mode),\n",
    "                nn.Conv2d(in_channels=c_in, out_channels=c_out,\n",
    "                          kernel_size=3, stride=1, padding=padding,\n",
    "                          bias=False\n",
    "                          )\n",
    "                )]\n",
    "        else:\n",
    "            block += [nn.ConvTranspose2d(in_channels=c_in,\n",
    "                                         out_channels=c_out,\n",
    "                                         kernel_size=kernel_size,\n",
    "                                         stride=stride,\n",
    "                                         padding=padding, bias=False\n",
    "                                         )\n",
    "                     ]\n",
    "\n",
    "        block += [nn.BatchNorm2d(num_features=c_out)]\n",
    "\n",
    "        if use_dropout:\n",
    "            block += [nn.Dropout(0.5)]\n",
    "\n",
    "        block += [nn.ReLU()]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEahjnYjVQuK"
   },
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHZr5kZDVQuK"
   },
   "source": [
    "Now that we’ve sorted out the building blocks, we can move on to implementing the network modules. **Pix2Pix** uses a **U-Net** architecture for its generator. The generator first reduces the input image to a low-dimensional latent space and then reconstructs it back to the original high-dimensional space. The discriminator is a **PatchGAN**. **PatchGAN** divides the input image into small patches and attempts to classify each patch as either real or fake. The discriminator is applied convolutionally across the entire image and then takes the average of all responses to produce the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_98AGJ_NVQuL"
   },
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn32UUa5VQuL"
   },
   "source": [
    "The generator follows a **U-Net**-like architecture. Key features of this architecture include the **encoder-decoder** structure and **skip connections**.\n",
    "\n",
    "1. The **encoder** is a subnetwork where the input image is encoded into a low-dimensional representation.\n",
    "2. The **decoder** is similar to the encoder but takes the low-dimensional representation and reconstructs it back to the original spatial dimensions.\n",
    "3. **Skip connections** transfer activations from corresponding layers in the encoder to the decoder.\n",
    "\n",
    "\n",
    "**Encoder**:\n",
    "\n",
    "The encoder consists of 8 downsampling blocks, with the following structure:\n",
    "- C64-C128-C256-C512-C512-C512-C512-C512\n",
    "- Convolutions use 4 × 4 filters with stride 2.\n",
    "- The encoder expects an input shape of (N,C,256,256).\n",
    "- At the end of the encoder, the output shape is (N,512,1,1).\n",
    "\n",
    "\n",
    "**Decoder**:\n",
    "\n",
    "The decoder consists of 8 upsampling blocks, with the following structure:\n",
    "- CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n",
    "- The number of channels is doubled in the decoder due to the skip connections.\n",
    "- The decoder expects an input shape of (N,512,1,1)\n",
    "- At the end of the decoder, the output shape is (N,64,256,256)\n",
    "\n",
    "After the last layer in the decoder, a convolution is applied to map to the desired number of output channels, followed by a **Tanh** activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Kjx3GrHVQuL"
   },
   "outputs": [],
   "source": [
    "class UnetEncoder(nn.Module):\n",
    "    \"\"\"Create the Unet Encoder Network.\n",
    "\n",
    "    C64-C128-C256-C512-C512-C512-C512-C512\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in=3, c_out=512):\n",
    "        \"\"\"\n",
    "        Constructs the Unet Encoder Network.\n",
    "\n",
    "        Ck denote a Convolution-BatchNorm-ReLU layer with k filters.\n",
    "            C64-C128-C256-C512-C512-C512-C512-C512\n",
    "        Args:\n",
    "            c_in (int, optional): Number of input channels.\n",
    "            c_out (int, optional): Number of output channels. Default is 512.\n",
    "        \"\"\"\n",
    "        super(UnetEncoder, self).__init__()\n",
    "        self.enc1 = DownsamplingBlock(c_in, 64, use_norm=False) # C64\n",
    "        self.enc2 = DownsamplingBlock(64, 128) # C128\n",
    "        self.enc3 = DownsamplingBlock(128, 256) # C256\n",
    "        self.enc4 = DownsamplingBlock(256, 512) # C512\n",
    "        self.enc5 = DownsamplingBlock(512, 512) # C512\n",
    "        self.enc6 = DownsamplingBlock(512, 512) # C512\n",
    "        self.enc7 = DownsamplingBlock(512, 512) # C512\n",
    "        self.enc8 = DownsamplingBlock(512, c_out) # C512\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.enc1(x)\n",
    "        x2 = self.enc2(x1)\n",
    "        x3 = self.enc3(x2)\n",
    "        x4 = self.enc4(x3)\n",
    "        x5 = self.enc5(x4)\n",
    "        x6 = self.enc6(x5)\n",
    "        x7 = self.enc7(x6)\n",
    "        x8 = self.enc8(x7)\n",
    "        out = [x8, x7, x6, x5, x4, x3, x2, x1] # latest activation is the first element\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wjgf8V8pVQuM"
   },
   "outputs": [],
   "source": [
    "class UnetDecoder(nn.Module):\n",
    "    \"\"\"Creates the Unet Decoder Network.\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in=512, c_out=64, use_upsampling=False, mode='nearest'):\n",
    "        \"\"\"\n",
    "        Constructs the Unet Decoder Network.\n",
    "\n",
    "        Ck denote a Convolution-BatchNorm-ReLU layer with k filters.\n",
    "\n",
    "        CDk denotes a Convolution-BatchNorm-Dropout-ReLU layer with a dropout rate of 50%.\n",
    "            CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n",
    "        Args:\n",
    "            c_in (int): Number of input channels.\n",
    "            c_out (int, optional): Number of output channels. Default is 512.\n",
    "            use_upsampling (bool, optional): Upsampling method for decoder.\n",
    "                If True, use upsampling layer followed regular convolution layer.\n",
    "                If False, use transpose convolution. Default is False\n",
    "            mode (str, optional): the upsampling algorithm: one of 'nearest',\n",
    "                'bilinear', 'bicubic'. Default: 'nearest'\n",
    "        \"\"\"\n",
    "        super(UnetDecoder, self).__init__()\n",
    "        self.dec1 = UpsamplingBlock(c_in, 512, use_dropout=True, use_upsampling=use_upsampling, mode=mode) # CD512\n",
    "        self.dec2 = UpsamplingBlock(1024, 512, use_dropout=True, use_upsampling=use_upsampling, mode=mode) # CD1024\n",
    "        self.dec3 = UpsamplingBlock(1024, 512, use_dropout=True, use_upsampling=use_upsampling, mode=mode) # CD1024\n",
    "        self.dec4 = UpsamplingBlock(1024, 512, use_upsampling=use_upsampling, mode=mode) # C1024\n",
    "        self.dec5 = UpsamplingBlock(1024, 256, use_upsampling=use_upsampling, mode=mode) # C1024\n",
    "        self.dec6 = UpsamplingBlock(512, 128, use_upsampling=use_upsampling, mode=mode) # C512\n",
    "        self.dec7 = UpsamplingBlock(256, 64, use_upsampling=use_upsampling, mode=mode) # C256\n",
    "        self.dec8 = UpsamplingBlock(128, c_out, use_upsampling=use_upsampling, mode=mode) # C128\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x9 = torch.cat([x[1], self.dec1(x[0])], 1) # (N,1024,H,W)\n",
    "        x10 = torch.cat([x[2], self.dec2(x9)], 1) # (N,1024,H,W)\n",
    "        x11 = torch.cat([x[3], self.dec3(x10)], 1) # (N,1024,H,W)\n",
    "        x12 = torch.cat([x[4], self.dec4(x11)], 1) # (N,1024,H,W)\n",
    "        x13 = torch.cat([x[5], self.dec5(x12)], 1) # (N,512,H,W)\n",
    "        x14 = torch.cat([x[6], self.dec6(x13)], 1) # (N,256,H,W)\n",
    "        x15 = torch.cat([x[7], self.dec7(x14)], 1) # (N,128,H,W)\n",
    "        out = self.dec8(x15) # (N,64,H,W)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AH4fhysGVQuM"
   },
   "outputs": [],
   "source": [
    "class UnetGenerator(nn.Module):\n",
    "    \"\"\"Create a Unet-based generator\"\"\"\n",
    "    def __init__(self, c_in=3, c_out=3, use_upsampling=False, mode='nearest'):\n",
    "        \"\"\"\n",
    "        Constructs a Unet generator\n",
    "        Args:\n",
    "            c_in (int): The number of input channels.\n",
    "            c_out (int): The number of output channels.\n",
    "            use_upsampling (bool, optional): Upsampling method for decoder.\n",
    "                If True, use upsampling layer followed regular convolution layer.\n",
    "                If False, use transpose convolution. Default is False\n",
    "            mode (str, optional): the upsampling algorithm: one of 'nearest',\n",
    "                'bilinear', 'bicubic'. Default: 'nearest'\n",
    "        \"\"\"\n",
    "        super(UnetGenerator, self).__init__()\n",
    "        self.encoder = UnetEncoder(c_in=c_in)\n",
    "        self.decoder = UnetDecoder(use_upsampling=use_upsampling, mode=mode)\n",
    "        # In the paper, the authors state:\n",
    "        #   \"\"\"\n",
    "        #       After the last layer in the decoder, a convolution is applied\n",
    "        #       to map to the number of output channels (3 in general, except\n",
    "        #       in colorization, where it is 2), followed by a Tanh function.\n",
    "        #   \"\"\"\n",
    "        # However, in the official Lua implementation, only a Tanh layer is applied.\n",
    "        # Therefore, I took the liberty of adding a convolutional layer with a\n",
    "        # kernel size of 3.\n",
    "        # For more information please check the paper and official github repo:\n",
    "        # https://github.com/phillipi/pix2pix\n",
    "        # https://arxiv.org/abs/1611.07004\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=c_out,\n",
    "                      kernel_size=3, stride=1, padding=1,\n",
    "                      bias=True\n",
    "                      ),\n",
    "            nn.Tanh()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outE = self.encoder(x)\n",
    "        outD = self.decoder(outE)\n",
    "        out = self.head(outD)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH0Wv3LsVQuN"
   },
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Spd5MrnlVQuN"
   },
   "source": [
    "As previously mentioned, the discriminator in **Pix2Pix** is a **PatchGAN**. **PatchGAN** divides the input image into small patches and classifies each patch as either real or fake. This is achieved by applying a series of convolutional blocks, rather than evaluating the entire patch at once. With each convolution, the receptive field of the network grows, allowing it to make decisions based on increasingly larger portions of the patch. As a result, the number of layers in the **PatchGAN** varies depending on the patch size. In the original Pix2Pix paper, the authors found that a **70x70 PatchGAN** performs best.\n",
    "\n",
    "For more details about receptive fields, I recommend checking out another excellent blog from [distill.pub](https://distill.pub/):  \n",
    "[Computing Receptive Fields of Convolutional Neural Networks](https://distill.pub/2019/computing-receptive-fields/)\n",
    "\n",
    "A special case of **PatchGAN** is **PixelGAN**, where all convolutions are 1x1.\n",
    "\n",
    "**Discriminator Architectures**:\n",
    "\n",
    "- 70 × 70 discriminator architecture is:\n",
    "C64-C128-C256-C512\n",
    "\n",
    "- 1 × 1 discriminator:\n",
    "C64-C128\n",
    "\n",
    "- 16 × 16 discriminator:\n",
    "C64-C128\n",
    "\n",
    "- 286 × 286 discriminator:\n",
    "C64-C128-C256-C512-C512-C512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OmdOoQ6VQuN"
   },
   "outputs": [],
   "source": [
    "class PixelDiscriminator(nn.Module):\n",
    "    \"\"\"Create a PixelGAN discriminator (1x1 PatchGAN discriminator)\"\"\"\n",
    "    def __init__(self, c_in=3, c_hid=64):\n",
    "        \"\"\"Constructs a PixelGAN discriminator, a special form of PatchGAN Discriminator.\n",
    "        All convolutions are 1x1 spatial filters\n",
    "\n",
    "        Args:\n",
    "            c_in (int, optional): The number of input channels. Defaults to 3.\n",
    "            c_hid (int, optional): The number of channels after first conv layer.\n",
    "                Defaults to 64.\n",
    "        \"\"\"\n",
    "        super(PixelDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            DownsamplingBlock(c_in, c_hid, kernel_size=1, stride=1, padding=0, use_norm=False),\n",
    "            DownsamplingBlock(c_hid, c_hid*2, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Conv2d(in_channels=c_hid*2, out_channels=1, kernel_size=1)\n",
    "            )\n",
    "        # Similar to PatchDiscriminator, there should be a sigmoid layer at the end of discriminator.\n",
    "        # However, nn.BCEWithLogitsLoss combines the sigmoid layer with BCE loss,\n",
    "        # providing greater numerical stability. Therefore, the discriminator outputs\n",
    "        # logits to take advantage of this stability.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOTDKaQ7VQuO"
   },
   "outputs": [],
   "source": [
    "class PatchDiscriminator(nn.Module):\n",
    "    \"\"\"Create a PatchGAN discriminator\"\"\"\n",
    "    def __init__(self, c_in=3, c_hid=64, n_layers=3):\n",
    "        \"\"\"Constructs a PatchGAN discriminator\n",
    "\n",
    "        Args:\n",
    "            c_in (int, optional): The number of input channels. Defaults to 3.\n",
    "            c_hid (int, optional): The number of channels after first conv layer.\n",
    "                Defaults to 64.\n",
    "            n_layers (int, optional): the number of convolution blocks in the\n",
    "                discriminator. Defaults to 3.\n",
    "        \"\"\"\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "        model = [DownsamplingBlock(c_in, c_hid, use_norm=False)]\n",
    "\n",
    "        n_p = 1  # multiplier for previous channel\n",
    "        n_c = 1  # multiplier for current channel\n",
    "        # last block is with stride of 1, therefore iterate (n_layers-1) times\n",
    "        for n in range(1, n_layers):\n",
    "            n_p = n_c\n",
    "            n_c = min(2**n, 8)  # The number of channels is 512 at most\n",
    "\n",
    "            model += [DownsamplingBlock(c_hid*n_p, c_hid*n_c)]\n",
    "\n",
    "        n_p = n_c\n",
    "        n_c = min(2**n_layers, 8)\n",
    "        model += [DownsamplingBlock(c_hid*n_p, c_hid*n_c, stride=1)] # last block is with stride of 1\n",
    "\n",
    "        # last layer is a convolution followed by a Sigmoid function.\n",
    "        model += [nn.Conv2d(in_channels=c_hid*n_c, out_channels=1,\n",
    "                            kernel_size=4, stride=1, padding=1, bias=True\n",
    "                            )]\n",
    "        # Normally, there should be a sigmoid layer at the end of discriminator.\n",
    "        # However, nn.BCEWithLogitsLoss combines the sigmoid layer with BCE loss,\n",
    "        # providing greater numerical stability. Therefore, the discriminator outputs\n",
    "        # logits to take advantage of this stability.\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2EeBxiLVQuO"
   },
   "outputs": [],
   "source": [
    "class PatchGAN(nn.Module):\n",
    "    \"\"\"Create a PatchGAN discriminator\"\"\"\n",
    "    def __init__(self, c_in=3, c_hid=64, mode='patch', n_layers=3):\n",
    "        \"\"\"Constructs a PatchGAN discriminator.\n",
    "\n",
    "        Args:\n",
    "            c_in (int, optional): The number of input channels. Defaults to 3.\n",
    "            c_hid (int, optional): The number of channels after first\n",
    "                convolutional layer. Defaults to 64.\n",
    "            mode (str, optional): PatchGAN type. Use 'pixel' for PixelGAN, and\n",
    "                'patch' for other types. Defaults to 'patch'.\n",
    "            n_layers (int, optional): PatchGAN number of layers. Defaults to 3.\n",
    "                - 16x16 PatchGAN if n=1\n",
    "                - 34x34 PatchGAN if n=2\n",
    "                - 70x70 PatchGAN if n=3\n",
    "                - 142x142 PatchGAN if n=4\n",
    "                - 286x286 PatchGAN if n=5\n",
    "                - 574x574 PatchGAN if n=6\n",
    "        \"\"\"\n",
    "        super(PatchGAN, self).__init__()\n",
    "        if mode == 'pixel':\n",
    "            self.model = PixelDiscriminator(c_in, c_hid)\n",
    "        else:\n",
    "            self.model = PatchDiscriminator(c_in, c_hid, n_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qBpWmCPVQuP"
   },
   "source": [
    "### Pix2Pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kTJD-BiVQuP"
   },
   "source": [
    "It's time to combine everything to build the Pix2Pix network. The Pix2Pix module consists of a generator and a discriminator. However, it has additional functionalities to make things easier and cleaner. Since we are typically interested only in the generator, we initialize the discriminator only if we want to train the model. The Pix2Pix class takes care of preparing the losses and optimizers for us. We use the classic BCE Loss for adversarial training, and L1 loss is added additionally for the generator to help it produce higher quality images. The class also provides some additional methods for tasks like weight initialization and performing a training step. GANs are extremely sensitive to initialization. The parameters we used in the method are typically works well, and is also used by author of the pix2pix model. We have methods for performing both a single training step and a validation step. During the training step, the generator creates fake images, the discriminator is updated, and then the generator is updated. In the validation step, the same process occurs, but neither the generator nor the discriminator is updated; only the losses are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4xn3zj2VQuQ"
   },
   "outputs": [],
   "source": [
    "class Pix2Pix(nn.Module):\n",
    "    \"\"\"Create a Pix2Pix class. It is a model for image to image translation tasks.\n",
    "    By default, the model uses a Unet architecture for generator with transposed\n",
    "    convolution. The discriminator is 70x70 PatchGAN discriminator, by default.\n",
    "     \"\"\"\n",
    "    def __init__(self,\n",
    "                 c_in: int = 3,\n",
    "                 c_out: int = 3,\n",
    "                 is_train: bool = True,\n",
    "                 netD: str = 'patch',\n",
    "                 lambda_L1: float = 100.0,\n",
    "                 is_CGAN: bool = True,\n",
    "                 use_upsampling: bool = False,\n",
    "                 mode: str = 'nearest',\n",
    "                 c_hid: int = 64,\n",
    "                 n_layers: int = 3,\n",
    "                 lr: float = 0.0002,\n",
    "                 beta1: float = 0.5,\n",
    "                 beta2: float = 0.999\n",
    "                 ):\n",
    "        \"\"\"Constructs the Pix2Pix class.\n",
    "\n",
    "        Args:\n",
    "            c_in: Number of input channels\n",
    "            c_out: Number of output channels\n",
    "            is_train: Whether the model is in training mode\n",
    "            netD: Type of discriminator ('patch' or 'pixel')\n",
    "            lambda_L1: Weight for L1 loss\n",
    "            is_CGAN: If True, use conditional GAN architecture\n",
    "            use_upsampling: If True, use upsampling in generator instead of transpose conv\n",
    "            mode: Upsampling mode ('nearest', 'bilinear', 'bicubic')\n",
    "            c_hid: Number of base filters in discriminator\n",
    "            n_layers: Number of layers in discriminator\n",
    "            lr: Learning rate\n",
    "            beta1: Beta1 parameter for Adam optimizer\n",
    "            beta2: Beta2 parameter for Adam optimizer\n",
    "        \"\"\"\n",
    "        super(Pix2Pix, self).__init__()\n",
    "        self.is_CGAN = is_CGAN\n",
    "        self.lambda_L1 = lambda_L1\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.gen = UnetGenerator(c_in=c_in, c_out=c_out, use_upsampling=use_upsampling, mode=mode)\n",
    "        self.gen = self.gen.apply(self.weights_init)\n",
    "\n",
    "        if self.is_train:\n",
    "            # Conditional GANs need both input and output together, the total input channel is c_in+c_out\n",
    "            disc_in = c_in + c_out if is_CGAN else c_out\n",
    "            self.disc = PatchGAN(c_in=disc_in, c_hid=c_hid, mode=netD, n_layers=n_layers)\n",
    "            self.disc = self.disc.apply(self.weights_init)\n",
    "\n",
    "            # Initialize optimizers\n",
    "            self.gen_optimizer = torch.optim.Adam(\n",
    "                self.gen.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "            self.disc_optimizer = torch.optim.Adam(\n",
    "                self.disc.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "            # Initialize loss functions\n",
    "            self.criterion = nn.BCEWithLogitsLoss()\n",
    "            self.criterion_L1 = nn.L1Loss()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.gen(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def weights_init(m):\n",
    "        \"\"\"Initialize network weights.\n",
    "\n",
    "        Args:\n",
    "            m: network module\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _get_disc_inputs(self,\n",
    "                         real_images: torch.Tensor,\n",
    "                         target_images: torch.Tensor,\n",
    "                         fake_images: torch.Tensor\n",
    "                         ):\n",
    "        \"\"\"Prepare discriminator inputs based on conditional/unconditional setup.\"\"\"\n",
    "        if self.is_CGAN:\n",
    "            # Conditional GANs need both input and output together,\n",
    "            # Therefore, the total input channel is c_in+c_out\n",
    "            real_AB = torch.cat([real_images, target_images], dim=1)\n",
    "            fake_AB = torch.cat([real_images,\n",
    "                               fake_images.detach()],\n",
    "                               dim=1)\n",
    "        else:\n",
    "            real_AB = target_images\n",
    "            fake_AB = fake_images.detach()\n",
    "        return real_AB, fake_AB\n",
    "\n",
    "    def _get_gen_inputs(self,\n",
    "                        real_images: torch.Tensor,\n",
    "                        fake_images: torch.Tensor\n",
    "                        ):\n",
    "        \"\"\"Prepare discriminator inputs based on conditional/unconditional setup.\"\"\"\n",
    "        if self.is_CGAN:\n",
    "            # Conditional GANs need both input and output together,\n",
    "            # Therefore, the total input channel is c_in+c_out\n",
    "            fake_AB = torch.cat([real_images,\n",
    "                               fake_images],\n",
    "                               dim=1)\n",
    "        else:\n",
    "            fake_AB = fake_images\n",
    "        return fake_AB\n",
    "\n",
    "\n",
    "    def step_discriminator(self,\n",
    "                           real_images: torch.Tensor,\n",
    "                           target_images: torch.Tensor,\n",
    "                           fake_images: torch.Tensor\n",
    "                           ):\n",
    "        \"\"\"Discriminator forward/backward pass.\n",
    "\n",
    "        Args:\n",
    "            real_images: Input images\n",
    "            target_images: Ground truth images\n",
    "            fake_images: Generated images\n",
    "\n",
    "        Returns:\n",
    "            Discriminator loss value\n",
    "        \"\"\"\n",
    "        # Prepare inputs\n",
    "        real_AB, fake_AB = self._get_disc_inputs(real_images, target_images,\n",
    "                                                fake_images)\n",
    "\n",
    "        # Forward pass through the discriminator\n",
    "        pred_real = self.disc(real_AB) # D(x, y)\n",
    "        pred_fake = self.disc(fake_AB) # D(x, G(x))\n",
    "\n",
    "        # Compute the losses\n",
    "        lossD_real = self.criterion(pred_real, torch.ones_like(pred_real)) # (D(x, y), 1)\n",
    "        lossD_fake = self.criterion(pred_fake, torch.zeros_like(pred_fake)) # (D(x, y), 0)\n",
    "        lossD = (lossD_real + lossD_fake) * 0.5 # Combined Loss\n",
    "        return lossD\n",
    "\n",
    "    def step_generator(self,\n",
    "                       real_images: torch.Tensor,\n",
    "                       target_images: torch.Tensor,\n",
    "                       fake_images: torch.Tensor\n",
    "                       ):\n",
    "        \"\"\"Discriminator forward/backward pass.\n",
    "\n",
    "        Args:\n",
    "            real_images: Input images\n",
    "            target_images: Ground truth images\n",
    "            fake_images: Generated images\n",
    "\n",
    "        Returns:\n",
    "            Discriminator loss value\n",
    "        \"\"\"\n",
    "        # Prepare input\n",
    "        fake_AB = self._get_gen_inputs(real_images, fake_images)\n",
    "\n",
    "        # Forward pass through the discriminator\n",
    "        pred_fake = self.disc(fake_AB)\n",
    "\n",
    "        # Compute the losses\n",
    "        lossG_GaN = self.criterion(pred_fake, torch.ones_like(pred_fake)) # GAN Loss\n",
    "        lossG_L1 = self.criterion_L1(fake_images, target_images)           # L1 Loss\n",
    "        lossG = lossG_GaN + self.lambda_L1 * lossG_L1                      # Combined Loss\n",
    "        # Return total loss and individual components\n",
    "        return lossG, {\n",
    "            'loss_G': lossG.item(),\n",
    "            'loss_G_GAN': lossG_GaN.item(),\n",
    "            'loss_G_L1': lossG_L1.item()\n",
    "        }\n",
    "\n",
    "    def train_step(self,\n",
    "                   real_images: torch.Tensor,\n",
    "                   target_images: torch.Tensor\n",
    "                   ):\n",
    "        \"\"\"Performs a single training step.\n",
    "\n",
    "        Args:\n",
    "            real_images: Input images\n",
    "            target_images: Ground truth images\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing all loss values from this step\n",
    "        \"\"\"\n",
    "        # Forward pass through the generator\n",
    "        fake_images = self.forward(real_images)\n",
    "\n",
    "        # Update discriminator\n",
    "        self.disc_optimizer.zero_grad() # Reset the gradients for D\n",
    "        lossD = self.step_discriminator(real_images, target_images, fake_images) # Compute the loss\n",
    "        lossD.backward()\n",
    "        self.disc_optimizer.step() # Update D\n",
    "\n",
    "        # Update generator\n",
    "        self.gen_optimizer.zero_grad() # Reset the gradients for D\n",
    "        lossG, G_losses = self.step_generator(real_images, target_images, fake_images) # Compute the loss\n",
    "        lossG.backward()\n",
    "        self.gen_optimizer.step() # Update D\n",
    "\n",
    "        # Return all losses\n",
    "        return {\n",
    "            'loss_D': lossD.item(),\n",
    "            **G_losses\n",
    "        }\n",
    "\n",
    "    def validation_step(self,\n",
    "                   real_images: torch.Tensor,\n",
    "                   target_images: torch.Tensor\n",
    "                   ):\n",
    "        \"\"\"Performs a single validation step.\n",
    "\n",
    "        Args:\n",
    "            real_images: Input images\n",
    "            target_images: Ground truth images\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing all loss values from this step\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Forward pass through the generator\n",
    "            fake_images = self.forward(real_images)\n",
    "\n",
    "            # Compute the loss for D\n",
    "            lossD = self.step_discriminator(real_images, target_images, fake_images)\n",
    "\n",
    "            # Compute the loss for G\n",
    "            _, G_losses = self.step_generator(real_images, target_images, fake_images)\n",
    "\n",
    "        # Return all losses\n",
    "        return {\n",
    "            'loss_D': lossD.item(),\n",
    "            **G_losses\n",
    "        }\n",
    "\n",
    "    def generate(self,\n",
    "                 real_images: torch.Tensor,\n",
    "                 is_scaled: bool = False,\n",
    "                 to_uint8: bool = False\n",
    "                 ):\n",
    "        if not is_scaled:\n",
    "            real_images = real_images.to(dtype=torch.float32) # Make sure it's a float tensor\n",
    "            real_images = real_images / 255.0 # Normalize to [0, 1]\n",
    "        real_images = (real_images - 0.5) / 0.5 # Scale to [-1, 1]\n",
    "\n",
    "        with torch.no_grad(): # generate image\n",
    "            generated_images = self.forward(real_images)\n",
    "\n",
    "        generated_images = (generated_images + 1) / 2  # Rescale to [0, 1]\n",
    "        if to_uint8:\n",
    "            generated_images = (generated_images* 255).to(dtype=torch.uint8)  # Scale to [0, 255] and convert to uint8\n",
    "\n",
    "        return generated_images\n",
    "\n",
    "\n",
    "    def save_model(self, gen_path: str, disc_path: str = None):\n",
    "        \"\"\"\n",
    "        Saves the generator model's state dictionary to the specified path.\n",
    "        If in training mode and a discriminator path is provided, saves the\n",
    "        discriminator model's state dictionary as well.\n",
    "\n",
    "        Args:\n",
    "            gen_path (str): The file path where the generator model's state dictionary will be saved.\n",
    "            disc_path (str, optional): The file path where the discriminator model's state dictionary will be saved. Defaults to None.\n",
    "        \"\"\"\n",
    "        torch.save(self.gen.state_dict(), gen_path)\n",
    "        if self.is_train and disc_path is not None:\n",
    "            torch.save(self.disc.state_dict(), disc_path)\n",
    "\n",
    "    def load_model(self, gen_path: str, disc_path: str = None, device: str = None):\n",
    "        \"\"\"\n",
    "        Loads the generator and optionally the discriminator model from the specified file paths.\n",
    "\n",
    "        Args:\n",
    "            gen_path (str): Path to the generator model file.\n",
    "            disc_path (str, optional): Path to the discriminator model file. Defaults to None.\n",
    "            device (torch.device, optional): The device on which to load the models. If None, the device of the model's parameters will be used. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        device = device if device else next(self.gen.parameters()).device\n",
    "        self.gen.load_state_dict(torch.load(gen_path, map_location=device, weights_only=True), strict=False)\n",
    "        if disc_path is not None and self.is_train:\n",
    "            device = device if device else next(self.disc.parameters()).device\n",
    "            self.disc.load_state_dict(torch.load(gen_path, map_location=device, weights_only=True), strict=False)\n",
    "\n",
    "    def save_optimizer(self, gen_opt_path: str, disc_opt_path: str = None):\n",
    "        \"\"\"\n",
    "        Save the state of the optimizers to the specified file paths.\n",
    "        Args:\n",
    "            gen_opt_path (str): The file path to save the generator optimizer state.\n",
    "            disc_opt_path (str, optional): The file path to save the discriminator optimizer state. Defaults to None.\n",
    "        Notes:\n",
    "            This method saves the state of the generator optimizer to the specified `gen_opt_path`.\n",
    "            If `disc_opt_path` is provided, it also saves the state of the discriminator optimizer to the specified path.\n",
    "            This method only works if the model is in training mode (`is_train` is True). If the model is not in training mode,\n",
    "            it will print a message indicating that the model is not initialized in train mode.\n",
    "        \"\"\"\n",
    "        if self.is_train:\n",
    "            torch.save(self.gen_optimizer.state_dict(), gen_opt_path)\n",
    "            if disc_opt_path is not None:\n",
    "                torch.save(self.disc_optimizer.state_dict(), disc_opt_path)\n",
    "        else:\n",
    "            print('Model is initialized in train mode. See `is_train` for more.')\n",
    "\n",
    "    def load_optimizer(self, gen_opt_path: str, disc_opt_path: str = None):\n",
    "        \"\"\"\n",
    "        Loads the optimizer states for the generator and discriminator from the specified file paths.\n",
    "        Args:\n",
    "            gen_opt_path (str): Path to the file containing the generator optimizer state.\n",
    "            disc_opt_path (str, optional): Path to the file containing the discriminator optimizer state. Defaults to None.\n",
    "        Notes:\n",
    "            This method saves the state of the generator optimizer to the specified `gen_opt_path`.\n",
    "            If `disc_opt_path` is provided, it also saves the state of the discriminator optimizer to the specified path.\n",
    "            This method only works if the model is in training mode (`is_train` is True). If the model is not in training mode,\n",
    "            it will print a message indicating that the model is not initialized in train mode.\n",
    "        \"\"\"\n",
    "        if self.is_train:\n",
    "            self.gen_optimizer.load_state_dict(torch.load(gen_opt_path, weights_only=True))\n",
    "            if disc_opt_path is not None:\n",
    "                self.disc_optimizer.load_state_dict(torch.load(disc_opt_path, weights_only=True))\n",
    "        else:\n",
    "            print('Model is initialized in train mode. See `is_train` for more.')\n",
    "\n",
    "    def get_current_visuals(self,\n",
    "                            real_images: torch.Tensor,\n",
    "                            target_images: torch.Tensor\n",
    "                            ):\n",
    "        \"\"\"Return visualization images.\n",
    "\n",
    "        Args:\n",
    "            real_images: Input images\n",
    "            target_images: Ground truth images\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing input, target and generated images\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            fake_images = self.gen(real_images)\n",
    "        return {\n",
    "            'real': real_images,\n",
    "            'fake': fake_images,\n",
    "            'target': target_images\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv6UXt5jVQuR"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdmKzi-1SwIB"
   },
   "outputs": [],
   "source": [
    "# utils/config.py\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration class to handle YAML config files\"\"\"\n",
    "\n",
    "    def __init__(self, config_path: str, overrides: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"\n",
    "        Initialize configuration from YAML file with optional overrides\n",
    "\n",
    "        Args:\n",
    "            config_path: Path to YAML config file\n",
    "            overrides: Optional dictionary of values to override config\n",
    "        \"\"\"\n",
    "        self.config_path = Path(config_path)\n",
    "        if not self.config_path.exists():\n",
    "            raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "\n",
    "        # Load config file\n",
    "        with open(config_path) as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "\n",
    "        # Apply any overrides\n",
    "        if overrides:\n",
    "            self._override_config(overrides)\n",
    "\n",
    "        # Set up paths\n",
    "        self._setup_paths()\n",
    "\n",
    "    def _override_config(self, overrides: Dict[str, Any]):\n",
    "        \"\"\"Recursively override configuration values\"\"\"\n",
    "        def _update(d, u):\n",
    "            for k, v in u.items():\n",
    "                if isinstance(v, dict):\n",
    "                    d[k] = _update(d.get(k, {}), v)\n",
    "                else:\n",
    "                    d[k] = v\n",
    "            return d\n",
    "\n",
    "        _update(self.config, overrides)\n",
    "\n",
    "    def _setup_paths(self):\n",
    "        \"\"\"Setup output directories and experiment naming\"\"\"\n",
    "        # Create unique experiment name if not specified\n",
    "        if not self.config['logging']['comet']['name']:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            self.config['logging']['comet']['name'] = f\"pix2pix_{timestamp}\"\n",
    "\n",
    "        # Setup directories\n",
    "        for dir_name in ['checkpoint_dir', 'results_dir']:\n",
    "            path = Path(self.config['training'][dir_name])\n",
    "            path = path / self.config['logging']['comet']['name']\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "            self.config['training'][dir_name] = str(path)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.config[key]\n",
    "\n",
    "    def get(self, key, default=None):\n",
    "        \"\"\"Get config value with optional default\"\"\"\n",
    "        try:\n",
    "            return self[key]\n",
    "        except KeyError:\n",
    "            return default\n",
    "\n",
    "    def save(self, save_path: str):\n",
    "        \"\"\"Save current config to file\"\"\"\n",
    "        with open(save_path, 'w') as f:\n",
    "            yaml.dump(self.config, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0puk9rHSsiY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "# Function to extract features from Inception v3\n",
    "def extract_features(images, model):\n",
    "    # Ensure images are on the right device (CUDA or CPU)\n",
    "    images = images.cuda() if torch.cuda.is_available() else images\n",
    "    # Get the features (use the last pooling layer before classification)\n",
    "    with torch.no_grad():\n",
    "        features = model(images)\n",
    "    return features\n",
    "\n",
    "\n",
    "def calculate_fid(real_features, generated_features):\n",
    "    # Calculate the mean and covariance of real and generated feature distributions\n",
    "    mu_real = np.mean(real_features, axis=0)\n",
    "    mu_gen = np.mean(generated_features, axis=0)\n",
    "\n",
    "    # Calculate covariance matrices\n",
    "    sigma_real = np.cov(real_features, rowvar=False)\n",
    "    sigma_gen = np.cov(generated_features, rowvar=False)\n",
    "\n",
    "    # Compute the Fréchet distance between real and generated distributions\n",
    "    diff = mu_real - mu_gen\n",
    "    covmean = sqrtm(sigma_real.dot(sigma_gen))\n",
    "\n",
    "    # Numerically stable version of sqrtm\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid = diff.dot(diff) + np.trace(sigma_real + sigma_gen - 2 * covmean)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191,
     "referenced_widgets": [
      "06dc89f900d5460da59374e7651828ee",
      "c995e4057f184bb7bc118786c69e22e7",
      "1b670240f0e146b9b6f312953c847509",
      "32b253eb155f4bdc83fb54bca9495a98",
      "52352c59b8574d4d932f039f1ff392b9",
      "4da8b22c1b7746ff91081ff61a437411",
      "0b927d85fcef407299c335aebedbad96",
      "75c3d089de6a46e5b0ff0ced13e8d7bd",
      "20ee04dc4d7549ce8acdbe91b7886c54",
      "7fc8a5687ebd43b6bf0c71d11996a0ea",
      "9902d2f794a54ec081cc47bb78ed7be9"
     ]
    },
    "id": "XFMJ_Pyabznb",
    "outputId": "5b72faee-6e0a-46a5-dfae-e5b7ea4a53ae"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Specify the model repository (e.g., \"username/model_name\")\n",
    "repo_id = \"yuulind/pix2pix-sar2rgb\"\n",
    "\n",
    "# Specify the file you want to download (e.g., model weights, config, etc.)\n",
    "# If it's a model file, you would specify the file name inside the repo\n",
    "file_name = \"pix2pix_gen_180.pth\"  # or another relevant file name\n",
    "\n",
    "# Download the file to the current working directory or specify a local path\n",
    "file_path = hf_hub_download(repo_id=repo_id, filename=file_name, cache_dir=\"/content/\")\n",
    "\n",
    "print(f\"Downloaded file to: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ldsucam4Soiu",
    "outputId": "5193a797-d1d3-4dd9-e117-6969581c611d"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation Script\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import models\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    # Load configuration\n",
    "    config = Config('/content/config.yaml')\n",
    "    # Set device\n",
    "    device = torch.device(config['training']['device'])\n",
    "\n",
    "    inception = models.inception_v3(weights='DEFAULT', transform_input=False).eval().to(device)\n",
    "\n",
    "    # transforms from inception_v3 documentation\n",
    "    transform = v2.Compose([\n",
    "        v2.Resize(342),\n",
    "        v2.CenterCrop(299),\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    dataset = Sentinel(\n",
    "        root_dir='/content/v_2',\n",
    "        split_type=\"test\",\n",
    "        split_mode=config['dataset']['split_mode'],\n",
    "        split_ratio=config['dataset']['split_ratio'],\n",
    "        split_file=config['dataset']['split_file'],\n",
    "        seed=config['dataset']['seed']\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=config['dataset']['shuffle'],\n",
    "        num_workers=config['training']['num_workers']\n",
    "    )\n",
    "\n",
    "    # Create model\n",
    "    model = Pix2Pix(\n",
    "        c_in=config['model']['c_in'],\n",
    "        c_out=config['model']['c_out'],\n",
    "        is_train=False,\n",
    "        use_upsampling=config['model']['use_upsampling'],\n",
    "        mode=config['model']['mode'],\n",
    "    ).to(device).eval()\n",
    "\n",
    "    gen_checkpoint = Path(file_path)\n",
    "\n",
    "    if not gen_checkpoint.exists():\n",
    "        raise FileNotFoundError(f\"Generator checkpoint file not found: {gen_checkpoint}\\nPlease check config.yaml\")\n",
    "\n",
    "    model.load_model(gen_path=gen_checkpoint)\n",
    "\n",
    "    target_features = []\n",
    "    fake_features = []\n",
    "\n",
    "    for real_images, target_images in dataloader:\n",
    "        real_images, target_images = real_images.to(device), target_images.to(device)\n",
    "\n",
    "        # Pix2Pix.generate() gets a scaled tensor ([0,1]) returns a uint8 tensor ([0,255])\n",
    "        fake_images = model.generate(real_images, is_scaled=True, to_uint8=True)\n",
    "\n",
    "        # Get target features\n",
    "        target_images = (target_images * 255).to(dtype=torch.uint8)\n",
    "        target_images = transform(target_images)\n",
    "        target_feats = extract_features(target_images, inception)\n",
    "        target_features.append(target_feats.cpu().numpy())\n",
    "\n",
    "        # Get fake features\n",
    "        fake_images = transform(fake_images)\n",
    "        fake_feats = extract_features(fake_images, inception)\n",
    "        fake_features.append(fake_feats.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    real_features = np.concatenate(target_features, axis=0)\n",
    "    generated_features = np.concatenate(fake_features, axis=0)\n",
    "\n",
    "    # Compute FID score\n",
    "    fid_score = calculate_fid(real_features, generated_features)\n",
    "    print(f\"FID Score: {fid_score}\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzAkhsuXjrEX"
   },
   "source": [
    "## Baseline FID: **31.20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMv0Ld9GWp6U",
    "outputId": "0b69fcd6-cc20-41ff-aa01-df072a76b7df"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation Script\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import models\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    inception = models.inception_v3(weights='DEFAULT', transform_input=False).eval().to(device)\n",
    "\n",
    "    # transforms from inception_v3 documentation\n",
    "    transform = v2.Compose([\n",
    "        v2.Resize(342),\n",
    "        v2.CenterCrop(299),\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    dataset = Sentinel(\n",
    "        root_dir='/content/v_2',\n",
    "        split_type=\"test\",\n",
    "        split_mode='random',\n",
    "        split_ratio=(0.0, 0.5, 0.5),\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    target_features = []\n",
    "\n",
    "    for real_images, target_images in dataloader:\n",
    "        real_images, target_images = real_images.to(device), target_images.to(device)\n",
    "\n",
    "        # Get target features\n",
    "        target_images = (target_images * 255).to(dtype=torch.uint8)\n",
    "        target_images = transform(target_images)\n",
    "        target_feats = extract_features(target_images, inception)\n",
    "        target_features.append(target_feats.cpu().numpy())\n",
    "\n",
    "    dataset = Sentinel(\n",
    "        root_dir='/content/v_2',\n",
    "        split_type=\"val\",\n",
    "        split_mode='random',\n",
    "        split_ratio=(0.0, 0.5, 0.5),\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    fake_features = []\n",
    "\n",
    "    for real_images, target_images in dataloader:\n",
    "        real_images, target_images = real_images.to(device), target_images.to(device)\n",
    "\n",
    "        # Get target features\n",
    "        target_images = (target_images * 255).to(dtype=torch.uint8)\n",
    "        target_images = transform(target_images)\n",
    "        target_feats = extract_features(target_images, inception)\n",
    "        fake_features.append(target_feats.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    real_features = np.concatenate(target_features, axis=0)\n",
    "    generated_features = np.concatenate(fake_features, axis=0)\n",
    "\n",
    "    # Compute FID score\n",
    "    fid_score = calculate_fid(real_features, generated_features)\n",
    "    print(f\"FID Score: {fid_score}\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvjeEoF9jll1"
   },
   "source": [
    "# Export To ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BgShHeu1aKa0",
    "outputId": "3a67225a-e363-4e66-8398-d383e6456f4d"
   },
   "outputs": [],
   "source": [
    "%pip install onnx onnxruntime -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsW3I8x6aodG"
   },
   "outputs": [],
   "source": [
    "model = Pix2Pix(is_train=False).eval()\n",
    "\n",
    "gen_checkpoint = file_path # Path('/content/pix2pix_gen_nn_150.pth')\n",
    "model.load_model(gen_path=gen_checkpoint)\n",
    "\n",
    "# Input to the model\n",
    "x = torch.randn(1, 3, 256, 256, requires_grad=True)\n",
    "torch_out = model(x)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"sar2rgb.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  #opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1ts8vsnbiB9"
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"/content/sar2rgb.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "CcV2NpcFcmHc",
    "outputId": "fc4291d0-8df4-4760-cf5b-1b016c74909a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"/content/sar2rgb.onnx\")\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "\n",
    "dataset = Sentinel(\n",
    "    root_dir='/content/v_2',\n",
    "    split_type='test',\n",
    "    transform=train_transforms,\n",
    "    split_mode='random',\n",
    "    split_ratio=(0.9,0.0,0.1),\n",
    "    seed=42\n",
    "    )\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    "    )\n",
    "\n",
    "reals, targets = next(iter(dataloader))\n",
    "\n",
    "with torch.no_grad():\n",
    "  fakes = model(reals)\n",
    "\n",
    "reals_cpu = reals.clone().detach().cpu().numpy()\n",
    "\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: reals_cpu}\n",
    "ort_outs = ort_session.run(None, ort_inputs)[0]\n",
    "\n",
    "def denormalize(tensor):\n",
    "    \"\"\"\n",
    "    Denormalize tensor from [-1, 1] to [0, 1] for visualization\n",
    "    \"\"\"\n",
    "    return (tensor + 1) / 2\n",
    "\n",
    "reals = denormalize(reals).numpy()\n",
    "targets = denormalize(targets).numpy()\n",
    "fakes = denormalize(fakes).numpy()\n",
    "\n",
    "ort_outs = (ort_outs + 1) / 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 6))\n",
    "axes[0].imshow(reals.squeeze(0).transpose((1,2,0)))\n",
    "axes[1].imshow(targets.squeeze(0).transpose((1,2,0)))\n",
    "axes[2].imshow(fakes.squeeze(0).transpose((1,2,0)))\n",
    "axes[3].imshow(ort_outs.squeeze(0).transpose((1,2,0)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwDUZhsXexxX"
   },
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EhbUu4se2tE"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "\n",
    "def normalize(image):\n",
    "    \"\"\"\n",
    "    Normalize image from [0, 255] to [-1, 1] for visualization\n",
    "    \"\"\"\n",
    "    image = image.astype(np.float32) / 255.0 # Normalize to [0, 1]\n",
    "    image = (image - 0.5) / 0.5 # Normalize to [-1, 1]\n",
    "    return image\n",
    "\n",
    "\n",
    "def denormalize(image):\n",
    "    \"\"\"\n",
    "    Denormalize image from [-1, 1] to [0, 1] for visualization\n",
    "    \"\"\"\n",
    "    return (image + 1) / 2\n",
    "\n",
    "def read_image(image_path):\n",
    "    \"\"\"\n",
    "    Read image from file path\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = np.array(image)\n",
    "    return image\n",
    "\n",
    "def prepare_image_gen(image):\n",
    "    \"\"\"\n",
    "    Prepare image for generation\n",
    "    \"\"\"\n",
    "    image = normalize(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = np.transpose(image, (0, 3, 1, 2))\n",
    "    return image\n",
    "\n",
    "def generate_image(image, ort_session):\n",
    "    \"\"\"\n",
    "    Predict image using ONNX runtime\n",
    "    \"\"\"\n",
    "    image = prepare_image_gen(image)\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: image}\n",
    "    generated_image = ort_session.run(None, ort_inputs)[0]\n",
    "    return generated_image\n",
    "\n",
    "def prepare_images_viz(image):\n",
    "    \"\"\"\n",
    "    Prepare images for visualization\n",
    "    \"\"\"\n",
    "    image = denormalize(image)\n",
    "    image = np.clip(image, 0, 1)\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    image = np.transpose(image, (0, 2, 3, 1)).squeeze(0)\n",
    "    return image\n",
    "\n",
    "def plot_images(input_images, real_images):\n",
    "    \"\"\"\n",
    "    Plot images\n",
    "    \"\"\"\n",
    "    n_images = len(input_images)\n",
    "\n",
    "    # Create figure with size and high quality\n",
    "    fig, axes = plt.subplots(3, n_images, figsize=(12, 8), dpi=300)\n",
    "\n",
    "    # Row labels\n",
    "    row_labels = ['SAR Image', 'Real RGB Image', 'Gen RGB Image']\n",
    "\n",
    "    # Column labels\n",
    "    column_labels = ['Agricultural Area', 'Barren Land Area', 'Grassland Area', 'Urban Area']\n",
    "\n",
    "    # ORT session\n",
    "    ort_session = onnxruntime.InferenceSession(\"/content/sar2rgb.onnx\")\n",
    "\n",
    "    for i, (input_img, real_img) in enumerate(zip(input_images, real_images)):\n",
    "        # Read image\n",
    "        input_image = read_image(input_img)\n",
    "        real_image = read_image(real_img)\n",
    "\n",
    "        # Generate output\n",
    "        generated_image = generate_image(input_image, ort_session)\n",
    "\n",
    "        # Put back into proper format\n",
    "        generated_image = prepare_images_viz(generated_image)\n",
    "\n",
    "        # visualize\n",
    "        axes[0, i].imshow(input_image)\n",
    "        axes[0, i].axis('off')  # Turn off ticks\n",
    "\n",
    "        # Plot Real Image\n",
    "        axes[1, i].imshow(real_image)\n",
    "        axes[1, i].axis('off')  # Turn off ticks\n",
    "\n",
    "        # Plot Generated Image\n",
    "        axes[2, i].imshow(generated_image)\n",
    "        axes[2, i].axis('off')  # Turn off ticks\n",
    "\n",
    "    # Set column labels\n",
    "    for ax, col_label in zip(axes[0], column_labels):\n",
    "        ax.set_title(col_label, fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Add row labels on the left of each row\n",
    "    for i, row_label in enumerate(row_labels):\n",
    "        axes[i, 0].text(-0.1, 0.5, row_label, va='center', ha='right', fontsize=14, fontweight='bold', transform=axes[i, 0].transAxes)\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and display the plot\n",
    "    plt.savefig('model_results.png', bbox_inches='tight')#, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuD8F_9xmci1"
   },
   "outputs": [],
   "source": [
    "input_images = ['v_2/agri/s1/ROIs1970_fall_s1_133_p1162.png',\n",
    "                'v_2/barrenland/s1/ROIs1970_fall_s1_71_p478.png',\n",
    "                'v_2/grassland/s1/ROIs1970_fall_s1_72_p665.png',\n",
    "                'v_2/urban/s1/ROIs1970_fall_s1_43_p667.png'\n",
    "                ]\n",
    "real_images = ['v_2/agri/s2/ROIs1970_fall_s2_133_p1162.png',\n",
    "               'v_2/barrenland/s2/ROIs1970_fall_s2_71_p478.png',\n",
    "               'v_2/grassland/s2/ROIs1970_fall_s2_72_p665.png',\n",
    "               'v_2/urban/s2/ROIs1970_fall_s2_43_p667.png'\n",
    "               ]\n",
    "\n",
    "plot_images(input_images, real_images)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "oKCCYgFFVQt_",
    "wonoYmX9VQuE",
    "Pe5xhNx7VQuH",
    "vEahjnYjVQuK",
    "_98AGJ_NVQuL",
    "lH0Wv3LsVQuN",
    "1qBpWmCPVQuP",
    "DzAkhsuXjrEX"
   ],
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1201791,
     "sourceId": 2008381,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06dc89f900d5460da59374e7651828ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c995e4057f184bb7bc118786c69e22e7",
       "IPY_MODEL_1b670240f0e146b9b6f312953c847509",
       "IPY_MODEL_32b253eb155f4bdc83fb54bca9495a98"
      ],
      "layout": "IPY_MODEL_52352c59b8574d4d932f039f1ff392b9"
     }
    },
    "0b927d85fcef407299c335aebedbad96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b670240f0e146b9b6f312953c847509": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75c3d089de6a46e5b0ff0ced13e8d7bd",
      "max": 218246476,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_20ee04dc4d7549ce8acdbe91b7886c54",
      "value": 218246476
     }
    },
    "20ee04dc4d7549ce8acdbe91b7886c54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "32b253eb155f4bdc83fb54bca9495a98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fc8a5687ebd43b6bf0c71d11996a0ea",
      "placeholder": "​",
      "style": "IPY_MODEL_9902d2f794a54ec081cc47bb78ed7be9",
      "value": " 218M/218M [00:06&lt;00:00, 35.6MB/s]"
     }
    },
    "4da8b22c1b7746ff91081ff61a437411": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52352c59b8574d4d932f039f1ff392b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75c3d089de6a46e5b0ff0ced13e8d7bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fc8a5687ebd43b6bf0c71d11996a0ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9902d2f794a54ec081cc47bb78ed7be9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c995e4057f184bb7bc118786c69e22e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4da8b22c1b7746ff91081ff61a437411",
      "placeholder": "​",
      "style": "IPY_MODEL_0b927d85fcef407299c335aebedbad96",
      "value": "pix2pix_gen_180.pth: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
